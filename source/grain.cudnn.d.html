<!DOCTYPE html>
<html>
<head>
    <title>grain.cudnn source code</title>
    <meta charset="utf-8" />
    <meta content="width=device-width, initial-scale=1" name="viewport" />
        <link rel="stylesheet" href="../style.css" />
        <script type="text/javascript" src="../script.js"></script>

    
    <link rel="prefetch" href="../search-results.html" />
</head>
<body>
    <div id="page-header">
        <div id="logotype">
        <span>Documentation</span>
        <nav>
            <a href="https://github.com/ShigekiKarita/grain">GitHub</a>
            <a href="https://github.com/ShigekiKarita/grain/tree/master/example">Example</a>
            <a href="http://dlang.org/">Dlang.org</a>
        </nav>
        </div>

        <form action="search-docs.html" id="search">
            <input type="search" placeholder="Find a symbol name..." name="searchTerm" />
            <input type="submit" value="Go" />
        </form>
    </div>
    <div id="page-body">
        <div id="page-content">
        <pre class="d_code highlighted with-line-wrappers"><a id="L1" href="#L1" class="br">1 </a><span class="com">/**
<a id="L2" href="#L2" class="br">2 </a>   cuDNN high level wrapper for grain.autograd.Variable
<a id="L3" href="#L3" class="br">3 </a>
<a id="L4" href="#L4" class="br">4 </a>   TODO: support global workspace instead of frequent allocation
<a id="L5" href="#L5" class="br">5 </a> */</span>
<a id="L6" href="#L6" class="br">6 </a><span class="kwrd">module</span> <span class="hid">grain.cudnn</span>;
<a id="L7" href="#L7" class="br">7 </a>
<a id="L8" href="#L8" class="br">8 </a><span class="kwrd">version</span> (<span class="hid">grain_cuda</span>):
<a id="L9" href="#L9" class="br">9 </a>
<a id="L10" href="#L10" class="br">10 </a><span class="kwrd">public</span> <span class="kwrd">import</span> <span class="hid">grain.cuda</span> : <span class="hid">cudnnHandle</span>, <span class="hid">checkCUDNN</span>, <span class="hid">CuPtr</span>, <span class="hid">CuArray</span>, <span class="hid">isDeviceMemory</span>;
<a id="L11" href="#L11" class="br">11 </a><span class="kwrd">import</span> <span class="hid">grain.autograd</span>; <span class="com">//  : Variable, DeviceStorage;</span>
<a id="L12" href="#L12" class="br">12 </a><span class="kwrd">import</span> <span class="hid">grain.utility</span> : <span class="hid">castArray</span>;
<a id="L13" href="#L13" class="br">13 </a><span class="kwrd">public</span> <span class="kwrd">import</span> <span class="hid">derelict.cuda</span>;
<a id="L14" href="#L14" class="br">14 </a><span class="kwrd">public</span> <span class="kwrd">import</span> <span class="hid">derelict.cudnn7</span>;
<a id="L15" href="#L15" class="br">15 </a>
<a id="L16" href="#L16" class="br">16 </a><span class="com">// TODO make shared</span>
<a id="L17" href="#L17" class="br">17 </a><span class="kwrd">__gshared</span> <span class="type">bool</span> <a href="grain.cudnn.d.html#L17" title="grain.cudnn.deterministic" class="hid">deterministic</a> = <span class="kwrd">false</span>;
<a id="L18" href="#L18" class="br">18 </a><span class="kwrd">__gshared</span> <span class="type">bool</span> <a href="grain.cudnn.d.html#L18" title="grain.cudnn.nanProp" class="hid">nanProp</a> = <span class="kwrd">true</span>;
<a id="L19" href="#L19" class="br">19 </a>
<a id="L20" href="#L20" class="br">20 </a><span class="com">/// return global cudnn option</span>
<a id="L21" href="#L21" class="br">21 </a><span class="kwrd">auto</span> <a href="grain.cudnn.d.html#L21" title="grain.cudnn.isDeterministic" class="hid">isDeterministic</a>() {
<a id="L22" href="#L22" class="br">22 </a>    <span class="kwrd">return</span> <a href="grain.cudnn.d.html#L17" title="grain.cudnn.deterministic" class="hid">deterministic</a> ? <span class="hid">CUDNN_DETERMINISTIC</span> : <span class="hid">CUDNN_NON_DETERMINISTIC</span>;
<a id="L23" href="#L23" class="br">23 </a>}
<a id="L24" href="#L24" class="br">24 </a>
<a id="L25" href="#L25" class="br">25 </a><span class="com">/// ditto</span>
<a id="L26" href="#L26" class="br">26 </a><span class="kwrd">auto</span> <a href="grain.cudnn.d.html#L26" title="grain.cudnn.isNanProp" class="hid">isNanProp</a>() {
<a id="L27" href="#L27" class="br">27 </a>    <span class="kwrd">return</span> <a href="grain.cudnn.d.html#L18" title="grain.cudnn.nanProp" class="hid">nanProp</a> ? <span class="hid">CUDNN_PROPAGATE_NAN</span> : <span class="hid">CUDNN_NOT_PROPAGATE_NAN</span>;
<a id="L28" href="#L28" class="br">28 </a>}
<a id="L29" href="#L29" class="br">29 </a>
<a id="L30" href="#L30" class="br">30 </a>
<a id="L31" href="#L31" class="br">31 </a><span class="com">/// convert floating point types (float, double) into cudnn enum</span>
<a id="L32" href="#L32" class="br">32 </a><span class="kwrd">auto</span> <a href="grain.cudnn.d.html#L32" title="grain.cudnn.cudnnDataType" class="hid">cudnnDataType</a>(<span class="hid">T</span>)() {
<a id="L33" href="#L33" class="br">33 </a>    <span class="com">// TODO support half</span>
<a id="L34" href="#L34" class="br">34 </a>    <span class="kwrd">static</span> <span class="kwrd">if</span>(<span class="kwrd">is</span>(<span class="hid">T</span> == <span class="type">float</span>)) <span class="kwrd">return</span> <span class="hid">CUDNN_DATA_FLOAT</span>;
<a id="L35" href="#L35" class="br">35 </a>    <span class="kwrd">else</span> <span class="kwrd">static</span> <span class="kwrd">if</span>(<span class="kwrd">is</span>(<span class="hid">T</span> == <span class="type">double</span>)) <span class="kwrd">return</span> <span class="hid">CUDNN_DATA_DOUBLE</span>;
<a id="L36" href="#L36" class="br">36 </a>    <span class="kwrd">else</span> <span class="kwrd">static</span> <span class="kwrd">assert</span>(<span class="kwrd">false</span>, <span class="str">&quot;unsupported type&quot;</span>);
<a id="L37" href="#L37" class="br">37 </a>}
<a id="L38" href="#L38" class="br">38 </a>
<a id="L39" href="#L39" class="br">39 </a><span class="com">/// cudnn data type of variable like struct</span>
<a id="L40" href="#L40" class="br">40 </a><span class="kwrd">struct</span> <a href="grain.cudnn.d.html#L40" title="grain.cudnn.TensorDesc" class="hid">TensorDesc</a> {
<a id="L41" href="#L41" class="br">41 </a>    <span class="hid">cudnnTensorDescriptor_t</span> <span class="hid">desc</span>;
<a id="L42" href="#L42" class="br">42 </a>    <span class="hid">CUdeviceptr</span> <span class="hid">ptr</span>;
<a id="L43" href="#L43" class="br">43 </a>    <span class="kwrd">alias</span> <span class="hid">desc</span> <span class="kwrd">this</span>;
<a id="L44" href="#L44" class="br">44 </a>
<a id="L45" href="#L45" class="br">45 </a>    <span class="com">/// no copy</span>
<a id="L46" href="#L46" class="br">46 </a>    @<span class="hid">disable</span> <span class="kwrd">this</span>(<span class="kwrd">this</span>);
<a id="L47" href="#L47" class="br">47 </a>    <span class="com">/// no allocation on heap</span>
<a id="L48" href="#L48" class="br">48 </a>    @<span class="hid">disable</span> <span class="kwrd">new</span>(<span class="hid">size_t</span>);
<a id="L49" href="#L49" class="br">49 </a>
<a id="L50" href="#L50" class="br">50 </a>    ~<span class="kwrd">this</span>() {
<a id="L51" href="#L51" class="br">51 </a>        <span class="hid">checkCUDNN</span>( <span class="hid">cudnnDestroyTensorDescriptor</span>(<span class="hid">desc</span>) );
<a id="L52" href="#L52" class="br">52 </a>    }
<a id="L53" href="#L53" class="br">53 </a>}
<a id="L54" href="#L54" class="br">54 </a>
<a id="L55" href="#L55" class="br">55 </a><span class="com">/// convert variable to cudnn tensor discriptor object</span>
<a id="L56" href="#L56" class="br">56 </a><span class="kwrd">auto</span> <a href="grain.cudnn.d.html#L56" title="grain.cudnn.makeCudnnTensor" class="hid">makeCudnnTensor</a>(<span class="hid">T</span>, <span class="hid">size_t</span> <span class="hid">dim</span>)(<span class="hid">Variable</span>!(<span class="hid">T</span>, <span class="hid">dim</span>, <span class="hid">DeviceStorage</span>) <span class="hid">x</span>) {
<a id="L57" href="#L57" class="br">57 </a>    <span class="kwrd">static</span> <span class="kwrd">assert</span>(<span class="hid">dim</span> &lt; <span class="hid">CUDNN_DIM_MAX</span>);
<a id="L58" href="#L58" class="br">58 </a>    <span class="kwrd">static</span> <span class="kwrd">if</span> (<span class="hid">dim</span> &lt; <span class="num">4</span>) {
<a id="L59" href="#L59" class="br">59 </a>        <span class="kwrd">enum</span> <span class="type">int</span> <span class="hid">ddim</span> = <span class="num">4</span>;
<a id="L60" href="#L60" class="br">60 </a>        <span class="type">int</span>[<span class="hid">ddim</span>] <span class="hid">shape</span>;
<a id="L61" href="#L61" class="br">61 </a>        <span class="type">int</span>[<span class="hid">ddim</span>] <span class="hid">strides</span>;
<a id="L62" href="#L62" class="br">62 </a>        <span class="hid">shape</span>[] = <span class="num">1</span>;
<a id="L63" href="#L63" class="br">63 </a>        <span class="hid">strides</span>[] = <span class="num">1</span>;
<a id="L64" href="#L64" class="br">64 </a>        <span class="kwrd">foreach</span> (<span class="hid">d</span>; <span class="num">0</span> .. <span class="hid">dim</span>) {
<a id="L65" href="#L65" class="br">65 </a>            <span class="kwrd">assert</span>(<span class="hid">x.shape</span>[<span class="hid">d</span>] &lt; <span class="type">int</span>.<span class="hid">max</span>);
<a id="L66" href="#L66" class="br">66 </a>            <span class="hid">shape</span>[<span class="hid">d</span>] = <span class="kwrd">cast</span>(<span class="type">int</span>) <span class="hid">x.shape</span>[<span class="hid">d</span>];
<a id="L67" href="#L67" class="br">67 </a>        }
<a id="L68" href="#L68" class="br">68 </a>        <span class="com">// shape[0..dim] = x.shape;</span>
<a id="L69" href="#L69" class="br">69 </a>        <span class="hid">strides</span>[<span class="num">0</span>..<span class="hid">dim</span>] = <span class="hid">x.strides</span>;
<a id="L70" href="#L70" class="br">70 </a>    } <span class="kwrd">else</span> {
<a id="L71" href="#L71" class="br">71 </a>        <span class="kwrd">enum</span> <span class="type">int</span> <span class="hid">ddim</span> = <span class="kwrd">cast</span>(<span class="type">int</span>) <span class="hid">dim</span>;
<a id="L72" href="#L72" class="br">72 </a>        <span class="type">int</span>[<span class="hid">ddim</span>] <span class="hid">shape</span>;
<a id="L73" href="#L73" class="br">73 </a>        <span class="kwrd">foreach</span> (<span class="hid">d</span>; <span class="num">0</span> .. <span class="hid">dim</span>) {
<a id="L74" href="#L74" class="br">74 </a>            <span class="kwrd">assert</span>(<span class="hid">x.shape</span>[<span class="hid">d</span>] &lt; <span class="type">int</span>.<span class="hid">max</span>);
<a id="L75" href="#L75" class="br">75 </a>            <span class="hid">shape</span>[<span class="hid">d</span>] = <span class="kwrd">cast</span>(<span class="type">int</span>) <span class="hid">x.shape</span>[<span class="hid">d</span>];
<a id="L76" href="#L76" class="br">76 </a>        }
<a id="L77" href="#L77" class="br">77 </a>        <span class="kwrd">auto</span> <span class="hid">strides</span> = <span class="hid">x.strides</span>;
<a id="L78" href="#L78" class="br">78 </a>    }
<a id="L79" href="#L79" class="br">79 </a>
<a id="L80" href="#L80" class="br">80 </a>    <a href="grain.cudnn.d.html#L40" title="grain.cudnn.TensorDesc" class="hid">TensorDesc</a> <span class="hid">tdesc</span>;
<a id="L81" href="#L81" class="br">81 </a>    <span class="hid">tdesc.ptr</span> = <span class="hid">x.data.ptr</span>;
<a id="L82" href="#L82" class="br">82 </a>    <span class="hid">checkCUDNN</span>(<span class="hid">cudnnCreateTensorDescriptor</span>(&amp;<span class="hid">tdesc.desc</span>));
<a id="L83" href="#L83" class="br">83 </a>    <span class="hid">checkCUDNN</span>(<span class="hid">cudnnSetTensorNdDescriptor</span>(<span class="hid">tdesc.desc</span>,
<a id="L84" href="#L84" class="br">84 </a>                                          <a href="grain.cudnn.d.html#L32" title="grain.cudnn.cudnnDataType" class="hid">cudnnDataType</a>!<span class="hid">T</span>,
<a id="L85" href="#L85" class="br">85 </a>                                          <span class="hid">ddim</span>,
<a id="L86" href="#L86" class="br">86 </a>                                          <span class="hid">shape.ptr</span>,
<a id="L87" href="#L87" class="br">87 </a>                                          <span class="hid">strides.ptr</span>));
<a id="L88" href="#L88" class="br">88 </a>    <span class="kwrd">return</span> <span class="hid">tdesc</span>;
<a id="L89" href="#L89" class="br">89 </a>}
<a id="L90" href="#L90" class="br">90 </a>
<a id="L91" href="#L91" class="br">91 </a><span class="com">/// convert contiguous cuda storage to 1-D tensor disc</span>
<a id="L92" href="#L92" class="br">92 </a><span class="kwrd">auto</span> <a href="grain.cudnn.d.html#L56" title="grain.cudnn.makeCudnnTensor" class="hid">makeCudnnTensor</a>(<span class="hid">T</span>)(<span class="kwrd">ref</span> <span class="hid">T</span> <span class="hid">storage</span>) <span class="kwrd">if</span> (<span class="hid">isDeviceMemory</span>!<span class="hid">T</span>) {
<a id="L93" href="#L93" class="br">93 </a>    <span class="kwrd">import</span> <span class="hid">grain.cuda</span> : <span class="hid">CudaElementType</span>;
<a id="L94" href="#L94" class="br">94 </a>    <span class="kwrd">assert</span>(<span class="hid">storage.length</span> &lt;= <span class="type">int</span>.<span class="hid">max</span>);
<a id="L95" href="#L95" class="br">95 </a>    <span class="type">int</span>[<span class="num">1</span>] <span class="hid">shape</span> = [<span class="kwrd">cast</span>(<span class="type">int</span>) <span class="hid">storage.length</span>];
<a id="L96" href="#L96" class="br">96 </a>    <span class="type">int</span>[<span class="num">1</span>] <span class="hid">strides</span> = [<span class="num">1</span>];
<a id="L97" href="#L97" class="br">97 </a>    <span class="type">int</span> <span class="hid">ddim</span> = <span class="num">1</span>;
<a id="L98" href="#L98" class="br">98 </a>    <a href="grain.cudnn.d.html#L40" title="grain.cudnn.TensorDesc" class="hid">TensorDesc</a> <span class="hid">tdesc</span>;
<a id="L99" href="#L99" class="br">99 </a>    <span class="hid">tdesc.ptr</span> = <span class="hid">storage.ptr</span>;
<a id="L100" href="#L100" class="br">100 </a>    <span class="hid">checkCUDNN</span>(<span class="hid">cudnnCreateTensorDescriptor</span>(&amp;<span class="hid">tdesc.desc</span>));
<a id="L101" href="#L101" class="br">101 </a>    <span class="hid">checkCUDNN</span>(<span class="hid">cudnnSetTensorNdDescriptor</span>(<span class="hid">tdesc.desc</span>,
<a id="L102" href="#L102" class="br">102 </a>                                          <a href="grain.cudnn.d.html#L32" title="grain.cudnn.cudnnDataType" class="hid">cudnnDataType</a>!(<span class="hid">CudaElementType</span>!<span class="hid">T</span>),
<a id="L103" href="#L103" class="br">103 </a>                                          <span class="hid">ddim</span>,
<a id="L104" href="#L104" class="br">104 </a>                                          <span class="hid">shape.ptr</span>,
<a id="L105" href="#L105" class="br">105 </a>                                          <span class="hid">strides.ptr</span>));
<a id="L106" href="#L106" class="br">106 </a>    <span class="kwrd">return</span> <span class="hid">tdesc</span>;
<a id="L107" href="#L107" class="br">107 </a>}
<a id="L108" href="#L108" class="br">108 </a>
<a id="L109" href="#L109" class="br">109 </a>
<a id="L110" href="#L110" class="br">110 </a><span class="com">/// y = alpha * f(x) + beta * y</span>
<a id="L111" href="#L111" class="br">111 </a><span class="type">void</span> <a href="grain.cudnn.d.html#L111" title="grain.cudnn.activationForward" class="hid">activationForward</a>(<span class="hid">cudnnActivationMode_t</span> <span class="hid">A</span>, <span class="hid">T</span>, <span class="hid">size_t</span> <span class="hid">dim</span>)(
<a id="L112" href="#L112" class="br">112 </a>    <span class="hid">Variable</span>!(<span class="hid">T</span>, <span class="hid">dim</span>, <span class="hid">DeviceStorage</span>) <span class="hid">x</span>, <span class="hid">Variable</span>!(<span class="hid">T</span>, <span class="hid">dim</span>, <span class="hid">DeviceStorage</span>) <span class="hid">y</span>,
<a id="L113" href="#L113" class="br">113 </a>    <span class="hid">T</span> <span class="hid">alpha</span>=<span class="num">1.0</span>, <span class="hid">T</span> <span class="hid">beta</span>=<span class="num">0.0</span>, <span class="type">double</span> <span class="hid">coeff</span>=<span class="num">0.0</span>) {
<a id="L114" href="#L114" class="br">114 </a>    <span class="kwrd">static</span> <span class="kwrd">assert</span>(<span class="hid">dim</span> &lt;= <span class="num">5</span>, <span class="str">&quot;cuDNN only supports &lt;= 5 dim tensors. and pack dim is not supported yet.&quot;</span>);
<a id="L115" href="#L115" class="br">115 </a>    <span class="com">// init descriptors</span>
<a id="L116" href="#L116" class="br">116 </a>    <span class="hid">cudnnActivationDescriptor_t</span>  <span class="hid">activDesc</span>;
<a id="L117" href="#L117" class="br">117 </a>    <span class="hid">checkCUDNN</span>( <span class="hid">cudnnCreateActivationDescriptor</span>(&amp;<span class="hid">activDesc</span>) );
<a id="L118" href="#L118" class="br">118 </a>    <span class="kwrd">scope</span>(<span class="hid">exit</span>) <span class="hid">checkCUDNN</span>( <span class="hid">cudnnCreateActivationDescriptor</span>(&amp;<span class="hid">activDesc</span>) );
<a id="L119" href="#L119" class="br">119 </a>    <span class="hid">checkCUDNN</span>( <span class="hid">cudnnSetActivationDescriptor</span>(<span class="hid">activDesc</span>,
<a id="L120" href="#L120" class="br">120 </a>                                             <span class="hid">A</span>, <span class="com">// CUDNN_ACTIVATION_RELU,</span>
<a id="L121" href="#L121" class="br">121 </a>                                             <span class="hid">CUDNN_PROPAGATE_NAN</span>,
<a id="L122" href="#L122" class="br">122 </a>                                             <span class="hid">coeff</span>) );
<a id="L123" href="#L123" class="br">123 </a>    <span class="kwrd">auto</span> <span class="hid">tx</span> = <span class="hid">x.makeCudnnTensor</span>;
<a id="L124" href="#L124" class="br">124 </a>    <span class="kwrd">auto</span> <span class="hid">ty</span> = <span class="hid">y.makeCudnnTensor</span>;
<a id="L125" href="#L125" class="br">125 </a>    <span class="hid">checkCUDNN</span>( <span class="hid">cudnnActivationForward</span>(<span class="hid">cudnnHandle</span>,
<a id="L126" href="#L126" class="br">126 </a>                                       <span class="hid">activDesc</span>,
<a id="L127" href="#L127" class="br">127 </a>                                       &amp;<span class="hid">alpha</span>,
<a id="L128" href="#L128" class="br">128 </a>                                       <span class="hid">tx</span>,
<a id="L129" href="#L129" class="br">129 </a>                                       <span class="kwrd">cast</span>(<span class="type">void</span>*) <span class="hid">tx.ptr</span>,
<a id="L130" href="#L130" class="br">130 </a>                                       &amp;<span class="hid">beta</span>,
<a id="L131" href="#L131" class="br">131 </a>                                       <span class="hid">ty</span>,
<a id="L132" href="#L132" class="br">132 </a>                                       <span class="kwrd">cast</span>(<span class="type">void</span>*) <span class="hid">ty.ptr</span>) );
<a id="L133" href="#L133" class="br">133 </a>}
<a id="L134" href="#L134" class="br">134 </a>
<a id="L135" href="#L135" class="br">135 </a><span class="com">/// grad function of sigmoid/tanh ... etc wrapper</span>
<a id="L136" href="#L136" class="br">136 </a><span class="type">void</span> <a href="grain.cudnn.d.html#L136" title="grain.cudnn.activationBackward" class="hid">activationBackward</a>(<span class="hid">cudnnActivationMode_t</span> <span class="hid">A</span>, <span class="hid">T</span>, <span class="hid">size_t</span> <span class="hid">dim</span>)(
<a id="L137" href="#L137" class="br">137 </a>    <span class="hid">Variable</span>!(<span class="hid">T</span>, <span class="hid">dim</span>, <span class="hid">DeviceStorage</span>) <span class="hid">gx</span>, <span class="hid">Variable</span>!(<span class="hid">T</span>, <span class="hid">dim</span>, <span class="hid">DeviceStorage</span>) <span class="hid">gy</span>,
<a id="L138" href="#L138" class="br">138 </a>    <span class="hid">Variable</span>!(<span class="hid">T</span>, <span class="hid">dim</span>, <span class="hid">DeviceStorage</span>) <span class="hid">x</span>, <span class="hid">Variable</span>!(<span class="hid">T</span>, <span class="hid">dim</span>, <span class="hid">DeviceStorage</span>) <span class="hid">y</span>,
<a id="L139" href="#L139" class="br">139 </a>    <span class="hid">T</span> <span class="hid">alpha</span>=<span class="num">1.0</span>, <span class="hid">T</span> <span class="hid">beta</span>=<span class="num">0.0</span>, <span class="type">double</span> <span class="hid">coeff</span>=<span class="num">0.0</span>) {
<a id="L140" href="#L140" class="br">140 </a>    <span class="kwrd">static</span> <span class="kwrd">assert</span>(<span class="hid">dim</span> &lt;= <span class="num">5</span>, <span class="str">&quot;cuDNN only supports &lt;= 5 dim tensors. and pack dim is not supported yet.&quot;</span>);
<a id="L141" href="#L141" class="br">141 </a>    <span class="com">// init descriptors</span>
<a id="L142" href="#L142" class="br">142 </a>    <span class="hid">cudnnActivationDescriptor_t</span>  <span class="hid">activDesc</span>;
<a id="L143" href="#L143" class="br">143 </a>    <span class="hid">checkCUDNN</span>( <span class="hid">cudnnCreateActivationDescriptor</span>(&amp;<span class="hid">activDesc</span>) );
<a id="L144" href="#L144" class="br">144 </a>    <span class="kwrd">scope</span>(<span class="hid">exit</span>) <span class="hid">checkCUDNN</span>( <span class="hid">cudnnCreateActivationDescriptor</span>(&amp;<span class="hid">activDesc</span>) );
<a id="L145" href="#L145" class="br">145 </a>    <span class="hid">checkCUDNN</span>( <span class="hid">cudnnSetActivationDescriptor</span>(<span class="hid">activDesc</span>,
<a id="L146" href="#L146" class="br">146 </a>                                             <span class="hid">A</span>, <span class="com">// CUDNN_ACTIVATION_RELU,</span>
<a id="L147" href="#L147" class="br">147 </a>                                             <a href="grain.cudnn.d.html#L26" title="grain.cudnn.isNanProp" class="hid">isNanProp</a>(), <span class="com">// CUDNN_PROPAGATE_NAN,</span>
<a id="L148" href="#L148" class="br">148 </a>                                             <span class="hid">coeff</span>) );
<a id="L149" href="#L149" class="br">149 </a>    <span class="kwrd">auto</span> <span class="hid">tgx</span> = <span class="hid">gx.makeCudnnTensor</span>;
<a id="L150" href="#L150" class="br">150 </a>    <span class="kwrd">auto</span> <span class="hid">tgy</span> = <span class="hid">gy.makeCudnnTensor</span>;
<a id="L151" href="#L151" class="br">151 </a>    <span class="kwrd">auto</span> <span class="hid">tx</span> = <span class="hid">x.makeCudnnTensor</span>;
<a id="L152" href="#L152" class="br">152 </a>    <span class="kwrd">auto</span> <span class="hid">ty</span> = <span class="hid">y.makeCudnnTensor</span>;
<a id="L153" href="#L153" class="br">153 </a>    <span class="hid">checkCUDNN</span>( <span class="hid">cudnnActivationBackward</span>(<span class="hid">cudnnHandle</span>,
<a id="L154" href="#L154" class="br">154 </a>                                        <span class="hid">activDesc</span>,
<a id="L155" href="#L155" class="br">155 </a>                                        &amp;<span class="hid">alpha</span>,
<a id="L156" href="#L156" class="br">156 </a>                                        <span class="hid">ty</span>,
<a id="L157" href="#L157" class="br">157 </a>                                        <span class="kwrd">cast</span>(<span class="type">void</span>*) <span class="hid">ty.ptr</span>,
<a id="L158" href="#L158" class="br">158 </a>                                        <span class="hid">tgy</span>,
<a id="L159" href="#L159" class="br">159 </a>                                        <span class="kwrd">cast</span>(<span class="type">void</span>*) <span class="hid">tgy.ptr</span>,
<a id="L160" href="#L160" class="br">160 </a>                                        <span class="hid">tx</span>,
<a id="L161" href="#L161" class="br">161 </a>                                        <span class="kwrd">cast</span>(<span class="type">void</span>*) <span class="hid">tx.ptr</span>,
<a id="L162" href="#L162" class="br">162 </a>                                        &amp;<span class="hid">beta</span>,
<a id="L163" href="#L163" class="br">163 </a>                                        <span class="hid">tgx</span>,
<a id="L164" href="#L164" class="br">164 </a>                                        <span class="kwrd">cast</span>(<span class="type">void</span>*) <span class="hid">tgx.ptr</span>,
<a id="L165" href="#L165" class="br">165 </a>                    ) );
<a id="L166" href="#L166" class="br">166 </a>}
<a id="L167" href="#L167" class="br">167 </a>
<a id="L168" href="#L168" class="br">168 </a><span class="com">/// compute the softmax over all C for each H, W, N</span>
<a id="L169" href="#L169" class="br">169 </a><span class="type">void</span> <a href="grain.cudnn.d.html#L169" title="grain.cudnn.softmaxForward" class="hid">softmaxForward</a>(<span class="hid">cudnnSoftmaxAlgorithm_t</span> <span class="hid">A</span>, <span class="hid">T</span>, <span class="hid">size_t</span> <span class="hid">dim</span>)(
<a id="L170" href="#L170" class="br">170 </a>    <span class="hid">Variable</span>!(<span class="hid">T</span>, <span class="hid">dim</span>, <span class="hid">DeviceStorage</span>) <span class="hid">x</span>, <span class="hid">Variable</span>!(<span class="hid">T</span>, <span class="hid">dim</span>, <span class="hid">DeviceStorage</span>) <span class="hid">y</span>, <span class="hid">T</span> <span class="hid">alpha</span>=<span class="num">1.0</span>, <span class="hid">T</span> <span class="hid">beta</span>=<span class="num">0.0</span>) {
<a id="L171" href="#L171" class="br">171 </a>    <span class="kwrd">static</span> <span class="kwrd">assert</span>(<span class="hid">dim</span> &lt;= <span class="num">4</span>, <span class="str">&quot;cuDNN only supports &lt;= 4 dim tensors. and pack dim is not supported yet.&quot;</span>);
<a id="L172" href="#L172" class="br">172 </a>    <span class="hid">checkCUDNN</span>( <span class="hid">cudnnSoftmaxForward</span>(<span class="hid">cudnnHandle</span>,
<a id="L173" href="#L173" class="br">173 </a>                                    <span class="hid">A</span>,
<a id="L174" href="#L174" class="br">174 </a>                                    <span class="hid">CUDNN_SOFTMAX_MODE_CHANNEL</span>,
<a id="L175" href="#L175" class="br">175 </a>                                    &amp;<span class="hid">alpha</span>,
<a id="L176" href="#L176" class="br">176 </a>                                    <span class="hid">x.makeCudnnTensor</span>,
<a id="L177" href="#L177" class="br">177 </a>                                    <span class="kwrd">cast</span>(<span class="type">void</span>*) <span class="hid">x.data.ptr</span>,
<a id="L178" href="#L178" class="br">178 </a>                                    &amp;<span class="hid">beta</span>,
<a id="L179" href="#L179" class="br">179 </a>                                    <span class="hid">y.makeCudnnTensor</span>,
<a id="L180" href="#L180" class="br">180 </a>                                    <span class="kwrd">cast</span>(<span class="type">void</span>*) <span class="hid">y.data.ptr</span>));
<a id="L181" href="#L181" class="br">181 </a>}
<a id="L182" href="#L182" class="br">182 </a>
<a id="L183" href="#L183" class="br">183 </a><span class="com">/// grad of softmax</span>
<a id="L184" href="#L184" class="br">184 </a><span class="type">void</span> <a href="grain.cudnn.d.html#L184" title="grain.cudnn.softmaxBackward" class="hid">softmaxBackward</a>(<span class="hid">cudnnSoftmaxAlgorithm_t</span> <span class="hid">A</span>, <span class="hid">T</span>, <span class="hid">size_t</span> <span class="hid">dim</span>)(
<a id="L185" href="#L185" class="br">185 </a>    <span class="hid">Variable</span>!(<span class="hid">T</span>, <span class="hid">dim</span>, <span class="hid">DeviceStorage</span>) <span class="hid">gx</span>, <span class="hid">Variable</span>!(<span class="hid">T</span>, <span class="hid">dim</span>, <span class="hid">DeviceStorage</span>) <span class="hid">gy</span>,
<a id="L186" href="#L186" class="br">186 </a>    <span class="hid">Variable</span>!(<span class="hid">T</span>, <span class="hid">dim</span>, <span class="hid">DeviceStorage</span>) <span class="hid">y</span>, <span class="hid">T</span> <span class="hid">alpha</span>=<span class="num">1.0</span>, <span class="hid">T</span> <span class="hid">beta</span>=<span class="num">0.0</span>) {
<a id="L187" href="#L187" class="br">187 </a>    <span class="kwrd">static</span> <span class="kwrd">assert</span>(<span class="hid">dim</span> &lt;= <span class="num">4</span>, <span class="str">&quot;cuDNN only supports &lt;= 4 dim tensors. and pack dim is not supported yet.&quot;</span>);
<a id="L188" href="#L188" class="br">188 </a>    <span class="hid">checkCUDNN</span>( <span class="hid">cudnnSoftmaxBackward</span>(<span class="hid">cudnnHandle</span>,
<a id="L189" href="#L189" class="br">189 </a>                                     <span class="hid">A</span>,
<a id="L190" href="#L190" class="br">190 </a>                                     <span class="hid">CUDNN_SOFTMAX_MODE_CHANNEL</span>,
<a id="L191" href="#L191" class="br">191 </a>                                     &amp;<span class="hid">alpha</span>,
<a id="L192" href="#L192" class="br">192 </a>                                     <span class="hid">y.makeCudnnTensor</span>,
<a id="L193" href="#L193" class="br">193 </a>                                     <span class="kwrd">cast</span>(<span class="kwrd">const</span> <span class="type">void</span>*) <span class="hid">y.data.ptr</span>,
<a id="L194" href="#L194" class="br">194 </a>                                     <span class="hid">gy.makeCudnnTensor</span>,
<a id="L195" href="#L195" class="br">195 </a>                                     <span class="kwrd">cast</span>(<span class="kwrd">const</span> <span class="type">void</span>*) <span class="hid">gy.data.ptr</span>,
<a id="L196" href="#L196" class="br">196 </a>                                     &amp;<span class="hid">beta</span>,
<a id="L197" href="#L197" class="br">197 </a>                                     <span class="hid">gx.makeCudnnTensor</span>,
<a id="L198" href="#L198" class="br">198 </a>                                     <span class="kwrd">cast</span>(<span class="type">void</span>*) <span class="hid">gx.data.ptr</span>
<a id="L199" href="#L199" class="br">199 </a>                    ));
<a id="L200" href="#L200" class="br">200 </a>}
<a id="L201" href="#L201" class="br">201 </a>
<a id="L202" href="#L202" class="br">202 </a><span class="com">/**
<a id="L203" href="#L203" class="br">203 </a>   Tensor operation : C = op( alpha1 * A, alpha2 * B ) + beta * C
<a id="L204" href="#L204" class="br">204 </a>
<a id="L205" href="#L205" class="br">205 </a>   - list of ops
<a id="L206" href="#L206" class="br">206 </a>    CUDNN_OP_TENSOR_ADD  = 0,
<a id="L207" href="#L207" class="br">207 </a>    CUDNN_OP_TENSOR_MUL  = 1,
<a id="L208" href="#L208" class="br">208 </a>    CUDNN_OP_TENSOR_MIN  = 2,
<a id="L209" href="#L209" class="br">209 </a>    CUDNN_OP_TENSOR_MAX  = 3,
<a id="L210" href="#L210" class="br">210 </a>    CUDNN_OP_TENSOR_SQRT = 4,
<a id="L211" href="#L211" class="br">211 </a>    CUDNN_OP_TENSOR_NOT  = 5,
<a id="L212" href="#L212" class="br">212 </a>
<a id="L213" href="#L213" class="br">213 </a>   B tensor is ignored for CUDNN_OP_TENSOR_SQRT, CUDNN_OP_TENSOR_NOT.
<a id="L214" href="#L214" class="br">214 </a>*/</span>
<a id="L215" href="#L215" class="br">215 </a><span class="type">void</span> <a href="grain.cudnn.d.html#L215" title="grain.cudnn.tensorOp" class="hid">tensorOp</a>(<span class="hid">cudnnOpTensorOp_t</span> <span class="hid">op</span>, <span class="hid">T</span>, <span class="hid">size_t</span> <span class="hid">dim</span>)(
<a id="L216" href="#L216" class="br">216 </a>    <span class="hid">Variable</span>!(<span class="hid">T</span>, <span class="hid">dim</span>, <span class="hid">DeviceStorage</span>) <span class="hid">c</span>, <span class="hid">Variable</span>!(<span class="hid">T</span>, <span class="hid">dim</span>, <span class="hid">DeviceStorage</span>) <span class="hid">a</span>, <span class="hid">Variable</span>!(<span class="hid">T</span>, <span class="hid">dim</span>, <span class="hid">DeviceStorage</span>) <span class="hid">b</span>,
<a id="L217" href="#L217" class="br">217 </a>    <span class="hid">T</span> <span class="hid">alpha1</span> = <span class="num">1</span>, <span class="hid">T</span> <span class="hid">alpha2</span> = <span class="num">1</span>, <span class="hid">T</span> <span class="hid">beta</span> = <span class="num">0</span>
<a id="L218" href="#L218" class="br">218 </a>) {
<a id="L219" href="#L219" class="br">219 </a>    <span class="kwrd">import</span> <span class="hid">grain.functions.common</span> : <span class="hid">broadcastable</span>;
<a id="L220" href="#L220" class="br">220 </a>    <span class="kwrd">assert</span>(<span class="hid">broadcastable</span>(<span class="hid">a</span>, <span class="hid">b</span>).<span class="hid">ok</span>);
<a id="L221" href="#L221" class="br">221 </a>    <span class="hid">cudnnOpTensorDescriptor_t</span> <span class="hid">opDisc</span>;
<a id="L222" href="#L222" class="br">222 </a>    <span class="hid">checkCUDNN</span>( <span class="hid">cudnnCreateOpTensorDescriptor</span>(&amp;<span class="hid">opDisc</span>) );
<a id="L223" href="#L223" class="br">223 </a>    <span class="kwrd">scope</span>(<span class="hid">exit</span>) <span class="hid">cudnnDestroyOpTensorDescriptor</span>(<span class="hid">opDisc</span>);
<a id="L224" href="#L224" class="br">224 </a>    <span class="hid">checkCUDNN</span>( <span class="hid">cudnnSetOpTensorDescriptor</span>(<span class="hid">opDisc</span>, <span class="hid">op</span>, <a href="grain.cudnn.d.html#L32" title="grain.cudnn.cudnnDataType" class="hid">cudnnDataType</a>!<span class="hid">T</span>, <a href="grain.cudnn.d.html#L26" title="grain.cudnn.isNanProp" class="hid">isNanProp</a>()) );
<a id="L225" href="#L225" class="br">225 </a>    <span class="hid">checkCUDNN</span>( <span class="hid">cudnnOpTensor</span>(<span class="hid">cudnnHandle</span>, <span class="hid">opDisc</span>,
<a id="L226" href="#L226" class="br">226 </a>                              &amp;<span class="hid">alpha1</span>, <span class="hid">a.makeCudnnTensor</span>, <span class="kwrd">cast</span>(<span class="kwrd">const</span> <span class="type">void</span>*) <span class="hid">a.data.ptr</span>,
<a id="L227" href="#L227" class="br">227 </a>                              &amp;<span class="hid">alpha2</span>, <span class="hid">b.makeCudnnTensor</span>, <span class="kwrd">cast</span>(<span class="kwrd">const</span> <span class="type">void</span>*) <span class="hid">b.data.ptr</span>,
<a id="L228" href="#L228" class="br">228 </a>                              &amp;<span class="hid">beta</span>, <span class="hid">c.makeCudnnTensor</span>, <span class="kwrd">cast</span>(<span class="type">void</span>*) <span class="hid">c.data.ptr</span>) );
<a id="L229" href="#L229" class="br">229 </a>}
<a id="L230" href="#L230" class="br">230 </a>
<a id="L231" href="#L231" class="br">231 </a><span class="com">/// x = alpha x</span>
<a id="L232" href="#L232" class="br">232 </a><span class="type">void</span> <a href="grain.cudnn.d.html#L232" title="grain.cudnn.scale" class="hid">scale</a>(<span class="hid">T</span>, <span class="hid">size_t</span> <span class="hid">dim</span>)(<span class="hid">Variable</span>!(<span class="hid">T</span>, <span class="hid">dim</span>, <span class="hid">DeviceStorage</span>) <span class="hid">x</span>, <span class="hid">T</span> <span class="hid">alpha</span>) {
<a id="L233" href="#L233" class="br">233 </a>    <span class="hid">checkCUDNN</span>( <span class="hid">cudnnScaleTensor</span>(<span class="hid">cudnnHandle</span>, <span class="hid">x.makeCudnnTensor</span>, <span class="kwrd">cast</span>(<span class="type">void</span>*) <span class="hid">x.data.ptr</span>, &amp;<span class="hid">alpha</span>) );
<a id="L234" href="#L234" class="br">234 </a>}
<a id="L235" href="#L235" class="br">235 </a>
<a id="L236" href="#L236" class="br">236 </a><span class="com">/**
<a id="L237" href="#L237" class="br">237 </a>   Tensor operation : C = reduce op( alpha * A ) + beta * C
<a id="L238" href="#L238" class="br">238 </a>
<a id="L239" href="#L239" class="br">239 </a>   - list of op
<a id="L240" href="#L240" class="br">240 </a>    CUDNN_REDUCE_TENSOR_ADD          = 0,
<a id="L241" href="#L241" class="br">241 </a>    CUDNN_REDUCE_TENSOR_MUL          = 1,
<a id="L242" href="#L242" class="br">242 </a>    CUDNN_REDUCE_TENSOR_MIN          = 2,
<a id="L243" href="#L243" class="br">243 </a>    CUDNN_REDUCE_TENSOR_MAX          = 3,
<a id="L244" href="#L244" class="br">244 </a>    CUDNN_REDUCE_TENSOR_AMAX         = 4,
<a id="L245" href="#L245" class="br">245 </a>    CUDNN_REDUCE_TENSOR_AVG          = 5,
<a id="L246" href="#L246" class="br">246 </a>    CUDNN_REDUCE_TENSOR_NORM1        = 6,
<a id="L247" href="#L247" class="br">247 </a>    CUDNN_REDUCE_TENSOR_NORM2        = 7,
<a id="L248" href="#L248" class="br">248 </a>    CUDNN_REDUCE_TENSOR_MUL_NO_ZEROS = 8,
<a id="L249" href="#L249" class="br">249 </a>
<a id="L250" href="#L250" class="br">250 </a>   The NaN propagation enum applies to only the min and max reduce ops;
<a id="L251" href="#L251" class="br">251 </a>   the other reduce ops propagate NaN as usual.
<a id="L252" href="#L252" class="br">252 </a>   The indices space is ignored for reduce ops other than min or max.
<a id="L253" href="#L253" class="br">253 </a>*/</span>
<a id="L254" href="#L254" class="br">254 </a><span class="type">void</span> <a href="grain.cudnn.d.html#L254" title="grain.cudnn.reduce" class="hid">reduce</a>(<span class="hid">cudnnReduceTensorOp_t</span> <span class="hid">op</span>, <span class="hid">T</span>, <span class="hid">size_t</span> <span class="hid">dim</span>)(
<a id="L255" href="#L255" class="br">255 </a>    <span class="hid">Variable</span>!(<span class="hid">T</span>, <span class="hid">dim</span>, <span class="hid">DeviceStorage</span>) <span class="hid">src</span>, <span class="hid">Variable</span>!(<span class="hid">T</span>, <span class="hid">dim</span>, <span class="hid">DeviceStorage</span>) <span class="hid">dst</span>, <span class="hid">T</span> <span class="hid">alpha</span>=<span class="num">1</span>, <span class="hid">T</span> <span class="hid">beta</span>=<span class="num">0</span>)
<a id="L256" href="#L256" class="br">256 </a>{
<a id="L257" href="#L257" class="br">257 </a>    <span class="com">// create tensor</span>
<a id="L258" href="#L258" class="br">258 </a>    <span class="kwrd">auto</span> <span class="hid">srcDesc</span> = <span class="hid">src.makeCudnnTensor</span>;
<a id="L259" href="#L259" class="br">259 </a>    <span class="kwrd">auto</span> <span class="hid">dstDesc</span> = <span class="hid">dst.makeCudnnTensor</span>;
<a id="L260" href="#L260" class="br">260 </a>
<a id="L261" href="#L261" class="br">261 </a>    <span class="com">// create descriptor</span>
<a id="L262" href="#L262" class="br">262 </a>    <span class="hid">cudnnReduceTensorDescriptor_t</span> <span class="hid">opDesc</span>;
<a id="L263" href="#L263" class="br">263 </a>    <span class="hid">checkCUDNN</span>( <span class="hid">cudnnCreateReduceTensorDescriptor</span>(&amp;<span class="hid">opDesc</span>) );
<a id="L264" href="#L264" class="br">264 </a>    <span class="kwrd">scope</span>(<span class="hid">exit</span>) <span class="hid">cudnnDestroyReduceTensorDescriptor</span>(<span class="hid">opDesc</span>);
<a id="L265" href="#L265" class="br">265 </a>    <span class="hid">checkCUDNN</span>( <span class="hid">cudnnSetReduceTensorDescriptor</span>(
<a id="L266" href="#L266" class="br">266 </a>                    <span class="hid">opDesc</span>, <span class="hid">op</span>, <a href="grain.cudnn.d.html#L32" title="grain.cudnn.cudnnDataType" class="hid">cudnnDataType</a>!<span class="hid">T</span>, <a href="grain.cudnn.d.html#L26" title="grain.cudnn.isNanProp" class="hid">isNanProp</a>(),
<a id="L267" href="#L267" class="br">267 </a>                    <span class="hid">CUDNN_REDUCE_TENSOR_NO_INDICES</span>, <span class="com">// CUDNN_REDUCE_TENSOR_FLATTENED_INDICES for backprop?</span>
<a id="L268" href="#L268" class="br">268 </a>                    <span class="hid">CUDNN_32BIT_INDICES</span> <span class="com">// only uint is supported in cudnn7</span>
<a id="L269" href="#L269" class="br">269 </a>                    ) );
<a id="L270" href="#L270" class="br">270 </a>
<a id="L271" href="#L271" class="br">271 </a>    <span class="com">// create indices (for backprop???)</span>
<a id="L272" href="#L272" class="br">272 </a>    <span class="hid">size_t</span> <span class="hid">indicesBytes</span>;
<a id="L273" href="#L273" class="br">273 </a>    <span class="hid">checkCUDNN</span>( <span class="hid">cudnnGetReductionIndicesSize</span>(<span class="hid">cudnnHandle</span>, <span class="hid">opDesc</span>, <span class="hid">srcDesc</span>, <span class="hid">dstDesc</span>, &amp;<span class="hid">indicesBytes</span>) );
<a id="L274" href="#L274" class="br">274 </a>    <span class="kwrd">auto</span> <span class="hid">indices</span> = <span class="hid">CuPtr</span>!<span class="type">uint</span>(<span class="hid">indicesBytes</span> / <span class="type">uint</span>.<span class="hid">sizeof</span>);
<a id="L275" href="#L275" class="br">275 </a>
<a id="L276" href="#L276" class="br">276 </a>    <span class="com">// create workspace</span>
<a id="L277" href="#L277" class="br">277 </a>    <span class="hid">size_t</span> <span class="hid">workspaceBytes</span>;
<a id="L278" href="#L278" class="br">278 </a>    <span class="hid">checkCUDNN</span>( <span class="hid">cudnnGetReductionWorkspaceSize</span>(<span class="hid">cudnnHandle</span>, <span class="hid">opDesc</span>, <span class="hid">srcDesc</span>, <span class="hid">dstDesc</span>, &amp;<span class="hid">workspaceBytes</span>) );
<a id="L279" href="#L279" class="br">279 </a>    <span class="kwrd">auto</span> <span class="hid">workspace</span> = <span class="hid">CuPtr</span>!<span class="type">byte</span>(<span class="hid">workspaceBytes</span>);
<a id="L280" href="#L280" class="br">280 </a>
<a id="L281" href="#L281" class="br">281 </a>    <span class="hid">checkCUDNN</span>( <span class="hid">cudnnReduceTensor</span>(
<a id="L282" href="#L282" class="br">282 </a>                    <span class="hid">cudnnHandle</span>, <span class="hid">opDesc</span>,
<a id="L283" href="#L283" class="br">283 </a>                    <span class="kwrd">cast</span>(<span class="type">void</span>*) <span class="hid">indices.ptr</span>, <span class="hid">indicesBytes</span>,
<a id="L284" href="#L284" class="br">284 </a>                    <span class="kwrd">cast</span>(<span class="type">void</span>*) <span class="hid">workspace.ptr</span>, <span class="hid">workspaceBytes</span>,
<a id="L285" href="#L285" class="br">285 </a>                    <span class="kwrd">cast</span>(<span class="kwrd">const</span> <span class="type">void</span>*) &amp;<span class="hid">alpha</span>, <span class="hid">srcDesc</span>, <span class="kwrd">cast</span>(<span class="kwrd">const</span> <span class="type">void</span>*) <span class="hid">srcDesc.ptr</span>,
<a id="L286" href="#L286" class="br">286 </a>                    <span class="kwrd">cast</span>(<span class="kwrd">const</span> <span class="type">void</span>*) &amp;<span class="hid">beta</span>, <span class="hid">dstDesc</span>, <span class="kwrd">cast</span>(<span class="type">void</span>*) <span class="hid">dstDesc.ptr</span>
<a id="L287" href="#L287" class="br">287 </a>                    ) );
<a id="L288" href="#L288" class="br">288 </a>}
<a id="L289" href="#L289" class="br">289 </a>
<a id="L290" href="#L290" class="br">290 </a><span class="com">/// x[] = value (WARNING: not tested)</span>
<a id="L291" href="#L291" class="br">291 </a><span class="type">void</span> <a href="grain.cudnn.d.html#L291" title="grain.cudnn.fill" class="hid">fill</a>(<span class="hid">T</span>, <span class="hid">size_t</span> <span class="hid">dim</span>)(<span class="hid">Variable</span>!(<span class="hid">T</span>, <span class="hid">dim</span>, <span class="hid">DeviceStorage</span>) <span class="hid">x</span>, <span class="hid">T</span> <span class="hid">value</span>) {
<a id="L292" href="#L292" class="br">292 </a>    <span class="hid">checkCUDNN</span>( <span class="hid">cudnnSetTensor</span>(<span class="hid">cudnnHandle</span>, <span class="hid">x.makeCudnnTensor</span>, <span class="kwrd">cast</span>(<span class="type">void</span>*) <span class="hid">x.data.ptr</span>, <span class="kwrd">cast</span>(<span class="kwrd">const</span> <span class="type">void</span>*) &amp;<span class="hid">value</span>) );
<a id="L293" href="#L293" class="br">293 </a>}
<a id="L294" href="#L294" class="br">294 </a>
<a id="L295" href="#L295" class="br">295 </a><span class="com">/// WIP</span>
<a id="L296" href="#L296" class="br">296 </a><span class="type">bool</span> <a href="grain.cudnn.d.html#L296" title="grain.cudnn.isContiguous" class="hid">isContiguous</a>(<span class="hid">T</span>, <span class="hid">size_t</span> <span class="hid">dim</span>, <span class="kwrd">alias</span> <span class="hid">Storage</span>)(<span class="hid">Variable</span>!(<span class="hid">T</span>, <span class="hid">dim</span>, <span class="hid">Storage</span>) <span class="hid">x</span>) {
<a id="L297" href="#L297" class="br">297 </a>    <span class="com">// FIXME reconsider this when I support reshape, reversed and transposed</span>
<a id="L298" href="#L298" class="br">298 </a>    <span class="type">bool</span> <span class="hid">ret</span> = <span class="hid">x.strides</span>[$-<span class="num">1</span>] == <span class="num">1</span>;
<a id="L299" href="#L299" class="br">299 </a>    <span class="type">int</span> <span class="hid">s</span> = <span class="num">1</span>;
<a id="L300" href="#L300" class="br">300 </a>    <span class="kwrd">foreach_reverse</span>(<span class="hid">i</span>; <span class="num">0</span>..<span class="hid">dim</span>-<span class="num">1</span>) {
<a id="L301" href="#L301" class="br">301 </a>        <span class="hid">ret</span> &amp;= <span class="hid">x.strides</span>[<span class="hid">i</span>] == <span class="hid">x.strides</span>[<span class="hid">i</span> + <span class="num">1</span>] * <span class="hid">x.shape</span>[<span class="hid">i</span>+<span class="num">1</span>];
<a id="L302" href="#L302" class="br">302 </a>    }
<a id="L303" href="#L303" class="br">303 </a>    <span class="kwrd">return</span> <span class="hid">ret</span>;
<a id="L304" href="#L304" class="br">304 </a>}
<a id="L305" href="#L305" class="br">305 </a>
<a id="L306" href="#L306" class="br">306 </a><span class="com">///</span>
<a id="L307" href="#L307" class="br">307 </a><span class="kwrd">unittest</span> {
<a id="L308" href="#L308" class="br">308 </a>    {
<a id="L309" href="#L309" class="br">309 </a>        <span class="kwrd">auto</span> <span class="hid">x</span> = [[<span class="num">0.1f</span>, <span class="num">0.2f</span>], [<span class="num">0.3f</span>, <span class="num">0.4f</span>]].<span class="hid">variable</span>;
<a id="L310" href="#L310" class="br">310 </a>        <span class="kwrd">assert</span>(<span class="hid">x.isContiguous</span>);
<a id="L311" href="#L311" class="br">311 </a>        <span class="hid">x.strides</span> = [<span class="num">2</span>, <span class="num">2</span>];
<a id="L312" href="#L312" class="br">312 </a>        <span class="kwrd">assert</span>(!<span class="hid">x.isContiguous</span>);
<a id="L313" href="#L313" class="br">313 </a>    }
<a id="L314" href="#L314" class="br">314 </a>    <span class="kwrd">version</span> (<span class="hid">grain_cuda</span>) {
<a id="L315" href="#L315" class="br">315 </a>        <span class="kwrd">auto</span> <span class="hid">x</span> = [[<span class="num">0.1f</span>, <span class="num">0.2f</span>], [<span class="num">0.3f</span>, <span class="num">0.4f</span>]].<span class="hid">variable.to</span>!<span class="hid">DeviceStorage</span>;
<a id="L316" href="#L316" class="br">316 </a>        <span class="kwrd">assert</span>(<span class="hid">x.isContiguous</span>);
<a id="L317" href="#L317" class="br">317 </a>        <span class="hid">x.strides</span> = [<span class="num">2</span>, <span class="num">2</span>];
<a id="L318" href="#L318" class="br">318 </a>        <span class="kwrd">assert</span>(!<span class="hid">x.isContiguous</span>);
<a id="L319" href="#L319" class="br">319 </a>    }
<a id="L320" href="#L320" class="br">320 </a>}
<a id="L321" href="#L321" class="br">321 </a>
<a id="L322" href="#L322" class="br">322 </a><span class="com">/// copy src to dst with broadcasting</span>
<a id="L323" href="#L323" class="br">323 </a><span class="type">void</span> <a href="grain.cudnn.d.html#L323" title="grain.cudnn.transform" class="hid">transform</a>(<span class="hid">T</span>, <span class="hid">size_t</span> <span class="hid">dim</span>)(<span class="hid">Variable</span>!(<span class="hid">T</span>, <span class="hid">dim</span>, <span class="hid">DeviceStorage</span>) <span class="hid">src</span>, <span class="kwrd">ref</span> <span class="hid">Variable</span>!(<span class="hid">T</span>, <span class="hid">dim</span>, <span class="hid">DeviceStorage</span>) <span class="hid">dst</span>, <span class="hid">T</span> <span class="hid">alpha</span>=<span class="num">1</span>, <span class="hid">T</span> <span class="hid">beta</span>=<span class="num">0</span>) {
<a id="L324" href="#L324" class="br">324 </a>    <span class="kwrd">assert</span>(<span class="hid">src.shape</span> == <span class="hid">dst.shape</span>);
<a id="L325" href="#L325" class="br">325 </a>
<a id="L326" href="#L326" class="br">326 </a>    <span class="kwrd">if</span> (<span class="hid">src.isContiguous</span> &amp;&amp; <span class="hid">dst.isContiguous</span> &amp;&amp; <span class="hid">beta</span> == <span class="num">1</span>) {
<a id="L327" href="#L327" class="br">327 </a>        <span class="kwrd">import</span> <span class="hid">grain.cuda</span> : <span class="hid">axpy</span>;
<a id="L328" href="#L328" class="br">328 </a>        <span class="hid">axpy</span>(<span class="hid">src.data</span>, <span class="hid">dst.data</span>, <span class="hid">alpha</span>);
<a id="L329" href="#L329" class="br">329 </a>        <span class="kwrd">return</span>;
<a id="L330" href="#L330" class="br">330 </a>    }
<a id="L331" href="#L331" class="br">331 </a>
<a id="L332" href="#L332" class="br">332 </a>    <span class="hid">checkCUDNN</span>(
<a id="L333" href="#L333" class="br">333 </a>        <span class="hid">cudnnTransformTensor</span>(
<a id="L334" href="#L334" class="br">334 </a>            <span class="hid">cudnnHandle</span>,
<a id="L335" href="#L335" class="br">335 </a>            <span class="kwrd">cast</span>(<span class="kwrd">const</span> <span class="type">void</span>*) &amp;<span class="hid">alpha</span>, <span class="hid">src.makeCudnnTensor</span>, <span class="kwrd">cast</span>(<span class="kwrd">const</span> <span class="type">void</span>*) <span class="hid">src.data.ptr</span>,
<a id="L336" href="#L336" class="br">336 </a>            <span class="kwrd">cast</span>(<span class="kwrd">const</span> <span class="type">void</span>*) &amp;<span class="hid">beta</span>, <span class="hid">dst.makeCudnnTensor</span>, <span class="kwrd">cast</span>(<span class="type">void</span>*) <span class="hid">dst.data.ptr</span>
<a id="L337" href="#L337" class="br">337 </a>            ) );
<a id="L338" href="#L338" class="br">338 </a>}
<a id="L339" href="#L339" class="br">339 </a>
<a id="L340" href="#L340" class="br">340 </a><span class="kwrd">auto</span> <a href="grain.cudnn.d.html#L340" title="grain.cudnn.contiguous" class="hid">contiguous</a>(<span class="hid">T</span>, <span class="hid">size_t</span> <span class="hid">dim</span>)(<span class="hid">Variable</span>!(<span class="hid">T</span>, <span class="hid">dim</span>, <span class="hid">DeviceStorage</span>) <span class="hid">x</span>) {
<a id="L341" href="#L341" class="br">341 </a>    <span class="kwrd">auto</span> <span class="hid">y</span> = <span class="hid">x.uninit</span>;
<a id="L342" href="#L342" class="br">342 </a>    <span class="hid">y.bprop</span> = <span class="hid">x.bprop</span>;
<a id="L343" href="#L343" class="br">343 </a>    <a href="grain.cudnn.d.html#L323" title="grain.cudnn.transform" class="hid">transform</a>(<span class="hid">x</span>, <span class="hid">y</span>);
<a id="L344" href="#L344" class="br">344 </a>    <span class="kwrd">return</span> <span class="hid">y</span>;
<a id="L345" href="#L345" class="br">345 </a>}
<a id="L346" href="#L346" class="br">346 </a>
<a id="L347" href="#L347" class="br">347 </a><span class="com">/// test cudnnTransformTensor with array ptr manipulations</span>
<a id="L348" href="#L348" class="br">348 </a><span class="kwrd">unittest</span> {
<a id="L349" href="#L349" class="br">349 </a>    <span class="kwrd">import</span> <span class="hid">std.stdio</span>;
<a id="L350" href="#L350" class="br">350 </a>    <span class="com">// skipping stride 2</span>
<a id="L351" href="#L351" class="br">351 </a>    {
<a id="L352" href="#L352" class="br">352 </a>        <span class="kwrd">auto</span> <span class="hid">x</span> = [<span class="num">1f</span>, <span class="num">0f</span>, <span class="num">2f</span>, <span class="num">0f</span>, <span class="num">3f</span>].<span class="hid">variable</span>;
<a id="L353" href="#L353" class="br">353 </a>        <span class="hid">x.strides</span> = [<span class="num">2</span>];
<a id="L354" href="#L354" class="br">354 </a>        <span class="hid">x.shape</span> = [<span class="num">3</span>];
<a id="L355" href="#L355" class="br">355 </a>        <span class="kwrd">auto</span> <span class="hid">y</span> = <span class="hid">x.to</span>!<span class="hid">DeviceStorage.contiguous.to</span>!<span class="hid">HostStorage</span>;
<a id="L356" href="#L356" class="br">356 </a>        <span class="kwrd">assert</span>(<span class="hid">y.data</span> == [<span class="num">1f</span>, <span class="num">2f</span>, <span class="num">3f</span>]);
<a id="L357" href="#L357" class="br">357 </a>        <span class="kwrd">assert</span>(<span class="hid">y.strides</span> == [<span class="num">1</span>]);
<a id="L358" href="#L358" class="br">358 </a>        <span class="kwrd">assert</span>(<span class="hid">y.shape</span> == [<span class="num">3</span>]);
<a id="L359" href="#L359" class="br">359 </a>    }
<a id="L360" href="#L360" class="br">360 </a>    <span class="com">// reverse skipping stride -2</span>
<a id="L361" href="#L361" class="br">361 </a>    {
<a id="L362" href="#L362" class="br">362 </a>        <span class="kwrd">auto</span> <span class="hid">x</span> = [<span class="num">1f</span>, <span class="num">0f</span>, <span class="num">2f</span>, <span class="num">0f</span>, <span class="num">3f</span>].<span class="hid">variable</span>;
<a id="L363" href="#L363" class="br">363 </a>        <span class="hid">x.strides</span> = [-<span class="num">2</span>];
<a id="L364" href="#L364" class="br">364 </a>        <span class="hid">x.shape</span> = [<span class="num">3</span>];
<a id="L365" href="#L365" class="br">365 </a>        <span class="kwrd">auto</span> <span class="hid">dx</span> = <span class="hid">x.to</span>!<span class="hid">DeviceStorage</span>;
<a id="L366" href="#L366" class="br">366 </a>        <span class="hid">dx.data.ptr</span> += <span class="num">4</span> * <span class="type">float</span>.<span class="hid">sizeof</span>;
<a id="L367" href="#L367" class="br">367 </a>        <span class="kwrd">scope</span>(<span class="hid">exit</span>) <span class="hid">dx.data.ptr</span> -= <span class="num">4</span> * <span class="type">float</span>.<span class="hid">sizeof</span>;
<a id="L368" href="#L368" class="br">368 </a>        <span class="kwrd">auto</span> <span class="hid">y</span> = <span class="hid">dx.contiguous.to</span>!<span class="hid">HostStorage</span>;
<a id="L369" href="#L369" class="br">369 </a>        <span class="kwrd">assert</span>(<span class="hid">y.data</span> == [<span class="num">3f</span>, <span class="num">2f</span>, <span class="num">1f</span>]);
<a id="L370" href="#L370" class="br">370 </a>        <span class="kwrd">assert</span>(<span class="hid">y.strides</span> == [<span class="num">1</span>]);
<a id="L371" href="#L371" class="br">371 </a>        <span class="kwrd">assert</span>(<span class="hid">y.shape</span> == [<span class="num">3</span>]);
<a id="L372" href="#L372" class="br">372 </a>    }
<a id="L373" href="#L373" class="br">373 </a>    <span class="com">// multi-dim transposed stride [3, 1]</span>
<a id="L374" href="#L374" class="br">374 </a>    {
<a id="L375" href="#L375" class="br">375 </a>        <span class="kwrd">auto</span> <span class="hid">x</span> = [[<span class="num">1f</span>, <span class="num">0f</span>, <span class="num">2f</span>],
<a id="L376" href="#L376" class="br">376 </a>                  [<span class="num">0f</span>, <span class="num">3f</span>, <span class="num">0f</span>]].<span class="hid">variable</span>;
<a id="L377" href="#L377" class="br">377 </a>        <span class="hid">x.strides</span> = [<span class="num">1</span>, <span class="num">3</span>];
<a id="L378" href="#L378" class="br">378 </a>        <span class="hid">x.shape</span> = [<span class="num">3</span>, <span class="num">2</span>];
<a id="L379" href="#L379" class="br">379 </a>        <span class="kwrd">auto</span> <span class="hid">dx</span> = <span class="hid">x.to</span>!<span class="hid">DeviceStorage</span>;
<a id="L380" href="#L380" class="br">380 </a>        <span class="kwrd">auto</span> <span class="hid">y</span> = <span class="hid">dx.contiguous.to</span>!<span class="hid">HostStorage</span>;
<a id="L381" href="#L381" class="br">381 </a>        <span class="kwrd">assert</span>(<span class="hid">y.sliced</span> == [[<span class="num">1f</span>, <span class="num">0f</span>], [<span class="num">0f</span>, <span class="num">3f</span>], [<span class="num">2f</span>, <span class="num">0f</span>]]);
<a id="L382" href="#L382" class="br">382 </a>        <span class="kwrd">assert</span>(<span class="hid">y.strides</span> == [<span class="num">2</span>, <span class="num">1</span>]);
<a id="L383" href="#L383" class="br">383 </a>        <span class="kwrd">assert</span>(<span class="hid">y.shape</span> == [<span class="num">3</span>, <span class="num">2</span>]);
<a id="L384" href="#L384" class="br">384 </a>    }
<a id="L385" href="#L385" class="br">385 </a>    <span class="com">// multi-dim skipping stride [3, 2]</span>
<a id="L386" href="#L386" class="br">386 </a>    {
<a id="L387" href="#L387" class="br">387 </a>        <span class="kwrd">auto</span> <span class="hid">x</span> = [[<span class="num">1f</span>, <span class="num">0f</span>, <span class="num">2f</span>],
<a id="L388" href="#L388" class="br">388 </a>                  [<span class="num">0f</span>, <span class="num">3f</span>, <span class="num">0f</span>]].<span class="hid">variable</span>;
<a id="L389" href="#L389" class="br">389 </a>        <span class="hid">x.strides</span> = [<span class="num">3</span>, <span class="num">2</span>];
<a id="L390" href="#L390" class="br">390 </a>        <span class="hid">x.shape</span> = [<span class="num">2</span>, <span class="num">2</span>];
<a id="L391" href="#L391" class="br">391 </a>        <span class="kwrd">auto</span> <span class="hid">dx</span> = <span class="hid">x.to</span>!<span class="hid">DeviceStorage</span>;
<a id="L392" href="#L392" class="br">392 </a>        <span class="kwrd">auto</span> <span class="hid">y</span> = <span class="hid">dx.contiguous.to</span>!<span class="hid">HostStorage</span>;
<a id="L393" href="#L393" class="br">393 </a>        <span class="kwrd">assert</span>(<span class="hid">y.sliced</span> == [[<span class="num">1f</span>, <span class="num">2f</span>],  [<span class="num">0f</span>, <span class="num">0f</span>]]);
<a id="L394" href="#L394" class="br">394 </a>        <span class="kwrd">assert</span>(<span class="hid">y.strides</span> == [<span class="num">2</span>, <span class="num">1</span>]);
<a id="L395" href="#L395" class="br">395 </a>        <span class="kwrd">assert</span>(<span class="hid">y.shape</span> == [<span class="num">2</span>, <span class="num">2</span>]);
<a id="L396" href="#L396" class="br">396 </a>    }
<a id="L397" href="#L397" class="br">397 </a>    <span class="com">// multi-dim transposed skipping stride [2, 3]</span>
<a id="L398" href="#L398" class="br">398 </a>    {
<a id="L399" href="#L399" class="br">399 </a>        <span class="kwrd">auto</span> <span class="hid">x</span> = [[<span class="num">1f</span>, <span class="num">0f</span>, <span class="num">2f</span>],
<a id="L400" href="#L400" class="br">400 </a>                  [<span class="num">0f</span>, <span class="num">3f</span>, <span class="num">0f</span>]].<span class="hid">variable</span>;
<a id="L401" href="#L401" class="br">401 </a>        <span class="hid">x.strides</span> = [<span class="num">2</span>, <span class="num">3</span>];
<a id="L402" href="#L402" class="br">402 </a>        <span class="hid">x.shape</span> = [<span class="num">2</span>, <span class="num">2</span>];
<a id="L403" href="#L403" class="br">403 </a>        <span class="kwrd">auto</span> <span class="hid">dx</span> = <span class="hid">x.to</span>!<span class="hid">DeviceStorage</span>;
<a id="L404" href="#L404" class="br">404 </a>        <span class="com">// dx.data.ptr += (2 * 3 - 1) * float.sizeof;</span>
<a id="L405" href="#L405" class="br">405 </a>        <span class="com">// scope(exit) dx.data.ptr -= (2 * 3 - 1) * float.sizeof;</span>
<a id="L406" href="#L406" class="br">406 </a>        <span class="kwrd">auto</span> <span class="hid">y</span> = <span class="hid">dx.contiguous.to</span>!<span class="hid">HostStorage</span>;
<a id="L407" href="#L407" class="br">407 </a>        <span class="kwrd">assert</span>(<span class="hid">y.sliced</span> == [[<span class="num">1f</span>, <span class="num">0f</span>],  [<span class="num">2f</span>, <span class="num">0f</span>]]);
<a id="L408" href="#L408" class="br">408 </a>        <span class="kwrd">assert</span>(<span class="hid">y.strides</span> == [<span class="num">2</span>, <span class="num">1</span>]);
<a id="L409" href="#L409" class="br">409 </a>        <span class="kwrd">assert</span>(<span class="hid">y.shape</span> == [<span class="num">2</span>, <span class="num">2</span>]);
<a id="L410" href="#L410" class="br">410 </a>    }
<a id="L411" href="#L411" class="br">411 </a>    <span class="com">// multi-dim transposed reverse skipping stride [-2, -3]</span>
<a id="L412" href="#L412" class="br">412 </a>    {
<a id="L413" href="#L413" class="br">413 </a>        <span class="kwrd">auto</span> <span class="hid">x</span> = [[<span class="num">1f</span>, <span class="num">0f</span>, <span class="num">2f</span>],
<a id="L414" href="#L414" class="br">414 </a>                  [<span class="num">0f</span>, <span class="num">3f</span>, <span class="num">0f</span>]].<span class="hid">variable</span>;
<a id="L415" href="#L415" class="br">415 </a>        <span class="hid">x.strides</span> = [-<span class="num">2</span>, -<span class="num">3</span>];
<a id="L416" href="#L416" class="br">416 </a>        <span class="hid">x.shape</span> = [<span class="num">2</span>, <span class="num">2</span>];
<a id="L417" href="#L417" class="br">417 </a>        <span class="kwrd">auto</span> <span class="hid">dx</span> = <span class="hid">x.to</span>!<span class="hid">DeviceStorage</span>;
<a id="L418" href="#L418" class="br">418 </a>        <span class="hid">dx.data.ptr</span> += (<span class="num">2</span> * <span class="num">3</span> - <span class="num">1</span>) * <span class="type">float</span>.<span class="hid">sizeof</span>;
<a id="L419" href="#L419" class="br">419 </a>        <span class="kwrd">scope</span>(<span class="hid">exit</span>) <span class="hid">dx.data.ptr</span> -= (<span class="num">2</span> * <span class="num">3</span> - <span class="num">1</span>) * <span class="type">float</span>.<span class="hid">sizeof</span>;
<a id="L420" href="#L420" class="br">420 </a>        <span class="kwrd">auto</span> <span class="hid">y</span> = <span class="hid">dx.contiguous.to</span>!<span class="hid">HostStorage</span>;
<a id="L421" href="#L421" class="br">421 </a>        <span class="kwrd">assert</span>(<span class="hid">y.sliced</span> == [[<span class="num">0f</span>, <span class="num">2f</span>],  [<span class="num">0f</span>, <span class="num">1f</span>]]);
<a id="L422" href="#L422" class="br">422 </a>        <span class="kwrd">assert</span>(<span class="hid">y.strides</span> == [<span class="num">2</span>, <span class="num">1</span>]);
<a id="L423" href="#L423" class="br">423 </a>        <span class="kwrd">assert</span>(<span class="hid">y.shape</span> == [<span class="num">2</span>, <span class="num">2</span>]);
<a id="L424" href="#L424" class="br">424 </a>    }
<a id="L425" href="#L425" class="br">425 </a>
<a id="L426" href="#L426" class="br">426 </a>}
<a id="L427" href="#L427" class="br">427 </a>
<a id="L428" href="#L428" class="br">428 </a><span class="com">/// wrapper of cudnnConvolutionForward for Variable</span>
<a id="L429" href="#L429" class="br">429 </a><span class="type">void</span> <a href="grain.cudnn.d.html#L429" title="grain.cudnn.convForward" class="hid">convForward</a>(<span class="type">bool</span> <span class="hid">isConv</span>, <span class="type">bool</span> <span class="hid">isNchw</span>, <span class="hid">T</span>, <span class="hid">size_t</span> <span class="hid">dim</span>, <span class="hid">size_t</span> <span class="hid">imDims</span>)
<a id="L430" href="#L430" class="br">430 </a>    (<span class="hid">Variable</span>!(<span class="hid">T</span>, <span class="hid">dim</span>, <span class="hid">DeviceStorage</span>) <span class="hid">input</span>,      <span class="com">// [N, CI, HI, WI]</span>
<a id="L431" href="#L431" class="br">431 </a>     <span class="hid">Variable</span>!(<span class="hid">T</span>, <span class="hid">dim</span>, <span class="hid">DeviceStorage</span>) <span class="hid">filter</span>,     <span class="com">// [CO, CI/G, KH, KW]</span>
<a id="L432" href="#L432" class="br">432 </a>     <span class="kwrd">ref</span> <span class="hid">Variable</span>!(<span class="hid">T</span>, <span class="hid">dim</span>, <span class="hid">DeviceStorage</span>) <span class="hid">output</span>, <span class="com">// [N, CO, HO, WO]</span>
<a id="L433" href="#L433" class="br">433 </a>     <span class="kwrd">const</span> <span class="type">int</span>[<span class="hid">imDims</span>]   <span class="hid">stride</span>,
<a id="L434" href="#L434" class="br">434 </a>     <span class="kwrd">const</span> <span class="type">int</span>[<span class="hid">imDims</span>]   <span class="hid">pad</span>,
<a id="L435" href="#L435" class="br">435 </a>     <span class="kwrd">const</span> <span class="type">int</span>[<span class="hid">imDims</span>]   <span class="hid">dilation</span>,
<a id="L436" href="#L436" class="br">436 </a>     <span class="type">int</span> <span class="hid">ngroup</span> = <span class="num">1</span>,
<a id="L437" href="#L437" class="br">437 </a>     <span class="hid">cudnnConvolutionFwdAlgo_t</span> <span class="hid">algo</span> = <span class="hid">CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM</span>,
<a id="L438" href="#L438" class="br">438 </a>     <span class="type">float</span> <span class="hid">alpha</span> = <span class="num">1</span>,
<a id="L439" href="#L439" class="br">439 </a>     <span class="type">float</span> <span class="hid">beta</span> = <span class="num">0</span>
<a id="L440" href="#L440" class="br">440 </a>        ) {
<a id="L441" href="#L441" class="br">441 </a>    <span class="kwrd">static</span> <span class="kwrd">assert</span>(<span class="hid">dim</span> &lt; <span class="hid">CUDNN_DIM_MAX</span>);
<a id="L442" href="#L442" class="br">442 </a>    <span class="kwrd">static</span> <span class="kwrd">assert</span>(<span class="hid">dim</span> == <span class="hid">imDims</span> + <span class="num">2</span>, <span class="str">&quot;dim should be like N(batch), C(channel) ~ dim(stride)&quot;</span>);
<a id="L443" href="#L443" class="br">443 </a>    <span class="kwrd">enum</span> <span class="hid">cudnnConvolutionMode_t</span> <span class="hid">mode</span> = <span class="hid">isConv</span> ? <span class="hid">CUDNN_CONVOLUTION</span> : <span class="hid">CUDNN_CROSS_CORRELATION</span>;
<a id="L444" href="#L444" class="br">444 </a>    <span class="kwrd">enum</span> <span class="hid">cudnnTensorFormat_t</span> <span class="hid">format</span> = <span class="hid">isNchw</span> ? <span class="hid">CUDNN_TENSOR_NCHW</span> : <span class="hid">CUDNN_TENSOR_NCHW</span>;
<a id="L445" href="#L445" class="br">445 </a>
<a id="L446" href="#L446" class="br">446 </a>    <span class="kwrd">static</span> <span class="kwrd">if</span> (<span class="hid">imDims</span> == <span class="num">1</span>) {
<a id="L447" href="#L447" class="br">447 </a>        <span class="kwrd">enum</span> <span class="hid">nbDim_</span> = <span class="num">4</span>;
<a id="L448" href="#L448" class="br">448 </a>        <span class="kwrd">enum</span> <span class="hid">imDim_</span> = <span class="num">2</span>;
<a id="L449" href="#L449" class="br">449 </a>        <span class="kwrd">const</span> <span class="hid">stride_</span> = <span class="hid">stride</span> ~ [<span class="num">1</span>];
<a id="L450" href="#L450" class="br">450 </a>        <span class="kwrd">const</span> <span class="hid">pad_</span> = <span class="hid">pad</span> ~ [<span class="num">0</span>];
<a id="L451" href="#L451" class="br">451 </a>        <span class="kwrd">const</span> <span class="hid">dilation_</span> = <span class="hid">dilation</span> ~ [<span class="num">1</span>];
<a id="L452" href="#L452" class="br">452 </a>        <span class="kwrd">const</span> <span class="hid">fshape_</span> = <span class="hid">filter.shape.castArray</span>!<span class="type">int</span> ~ [<span class="num">1</span>];
<a id="L453" href="#L453" class="br">453 </a>    } <span class="kwrd">else</span> {
<a id="L454" href="#L454" class="br">454 </a>        <span class="kwrd">enum</span> <span class="hid">nbDim_</span> = <span class="hid">dim</span>;
<a id="L455" href="#L455" class="br">455 </a>        <span class="kwrd">enum</span> <span class="hid">imDim_</span> = <span class="hid">imDims</span>;
<a id="L456" href="#L456" class="br">456 </a>        <span class="kwrd">const</span> <span class="hid">pad_</span> = <span class="hid">pad</span>;
<a id="L457" href="#L457" class="br">457 </a>        <span class="kwrd">const</span> <span class="hid">stride_</span> = <span class="hid">stride</span>;
<a id="L458" href="#L458" class="br">458 </a>        <span class="kwrd">const</span> <span class="hid">dilation_</span> = <span class="hid">dilation</span>;
<a id="L459" href="#L459" class="br">459 </a>        <span class="kwrd">const</span> <span class="hid">fshape_</span> = <span class="hid">filter.shape.castArray</span>!<span class="type">int</span>;
<a id="L460" href="#L460" class="br">460 </a>    }
<a id="L461" href="#L461" class="br">461 </a>
<a id="L462" href="#L462" class="br">462 </a>    <span class="com">// import std.stdio;</span>
<a id="L463" href="#L463" class="br">463 </a>    <span class="com">// writeln(&quot;stride:&quot;, stride_);</span>
<a id="L464" href="#L464" class="br">464 </a>    <span class="com">// writeln(&quot;pad:&quot;, pad_);</span>
<a id="L465" href="#L465" class="br">465 </a>    <span class="com">// writeln(&quot;dilation:&quot;, dilation_);</span>
<a id="L466" href="#L466" class="br">466 </a>
<a id="L467" href="#L467" class="br">467 </a>    <span class="com">// TODO cache these?</span>
<a id="L468" href="#L468" class="br">468 </a>    <span class="hid">cudnnFilterDescriptor_t</span> <span class="hid">cudnnFdesc</span>;
<a id="L469" href="#L469" class="br">469 </a>    <span class="hid">checkCUDNN</span>( <span class="hid">cudnnCreateFilterDescriptor</span>(&amp;<span class="hid">cudnnFdesc</span>) );
<a id="L470" href="#L470" class="br">470 </a>    <span class="kwrd">scope</span>(<span class="hid">exit</span>) <span class="hid">cudnnDestroyFilterDescriptor</span>(<span class="hid">cudnnFdesc</span>);
<a id="L471" href="#L471" class="br">471 </a>    <span class="hid">checkCUDNN</span>( <span class="hid">cudnnSetFilterNdDescriptor</span>(<span class="hid">cudnnFdesc</span>, <a href="grain.cudnn.d.html#L32" title="grain.cudnn.cudnnDataType" class="hid">cudnnDataType</a>!<span class="hid">T</span>, <span class="hid">format</span>,
<a id="L472" href="#L472" class="br">472 </a>                                           <span class="kwrd">cast</span>(<span class="type">int</span>) <span class="hid">nbDim_</span>, <span class="hid">fshape_.ptr</span>
<a id="L473" href="#L473" class="br">473 </a>                                           ) );
<a id="L474" href="#L474" class="br">474 </a>
<a id="L475" href="#L475" class="br">475 </a>    <span class="hid">cudnnConvolutionDescriptor_t</span> <span class="hid">cudnnConvDesc</span>;
<a id="L476" href="#L476" class="br">476 </a>    <span class="hid">checkCUDNN</span>( <span class="hid">cudnnCreateConvolutionDescriptor</span>(&amp;<span class="hid">cudnnConvDesc</span>) );
<a id="L477" href="#L477" class="br">477 </a>    <span class="kwrd">scope</span>(<span class="hid">exit</span>) <span class="hid">cudnnDestroyConvolutionDescriptor</span>(<span class="hid">cudnnConvDesc</span>);
<a id="L478" href="#L478" class="br">478 </a>    <span class="hid">checkCUDNN</span>( <span class="hid">cudnnSetConvolutionGroupCount</span>(<span class="hid">cudnnConvDesc</span>, <span class="hid">ngroup</span>) );
<a id="L479" href="#L479" class="br">479 </a>    <span class="hid">checkCUDNN</span>( <span class="hid">cudnnSetConvolutionNdDescriptor</span>(<span class="hid">cudnnConvDesc</span>, <span class="kwrd">cast</span>(<span class="type">int</span>) <span class="hid">imDim_</span>,
<a id="L480" href="#L480" class="br">480 </a>                                                <span class="hid">pad_.ptr</span>, <span class="hid">stride_.ptr</span>, <span class="hid">dilation_.ptr</span>,
<a id="L481" href="#L481" class="br">481 </a>                                                <span class="hid">mode</span>, <a href="grain.cudnn.d.html#L32" title="grain.cudnn.cudnnDataType" class="hid">cudnnDataType</a>!<span class="hid">T</span>
<a id="L482" href="#L482" class="br">482 </a>                                                ) );
<a id="L483" href="#L483" class="br">483 </a>
<a id="L484" href="#L484" class="br">484 </a>    <span class="kwrd">auto</span> <span class="hid">cudnnIdesc</span> = <span class="hid">input.makeCudnnTensor</span>;
<a id="L485" href="#L485" class="br">485 </a>    <span class="kwrd">auto</span> <span class="hid">cudnnOdesc</span> = <span class="hid">output.makeCudnnTensor</span>;
<a id="L486" href="#L486" class="br">486 </a>    <span class="hid">size_t</span> <span class="hid">workSpaceSize</span>;
<a id="L487" href="#L487" class="br">487 </a>    <span class="hid">checkCUDNN</span> ( <span class="hid">cudnnGetConvolutionForwardWorkspaceSize</span>
<a id="L488" href="#L488" class="br">488 </a>                 (<span class="hid">cudnnHandle</span>, <span class="hid">cudnnIdesc</span>, <span class="hid">cudnnFdesc</span>, <span class="hid">cudnnConvDesc</span>,
<a id="L489" href="#L489" class="br">489 </a>                  <span class="hid">cudnnOdesc</span>, <span class="hid">algo</span>, &amp;<span class="hid">workSpaceSize</span>) );
<a id="L490" href="#L490" class="br">490 </a>    <span class="kwrd">auto</span> <span class="hid">workSpace</span> = <span class="hid">CuPtr</span>!<span class="type">byte</span>(<span class="hid">workSpaceSize</span>);
<a id="L491" href="#L491" class="br">491 </a>
<a id="L492" href="#L492" class="br">492 </a>    <span class="hid">checkCUDNN</span> ( <span class="hid">cudnnConvolutionForward</span> (<span class="hid">cudnnHandle</span>,
<a id="L493" href="#L493" class="br">493 </a>                                             <span class="kwrd">cast</span>(<span class="kwrd">const</span> <span class="type">void</span>*) &amp;<span class="hid">alpha</span>,
<a id="L494" href="#L494" class="br">494 </a>                                             <span class="hid">cudnnIdesc</span>, <span class="kwrd">cast</span>(<span class="kwrd">const</span> <span class="type">void</span>*) <span class="hid">input.data.ptr</span>,
<a id="L495" href="#L495" class="br">495 </a>                                             <span class="hid">cudnnFdesc</span>, <span class="kwrd">cast</span>(<span class="kwrd">const</span> <span class="type">void</span>*) <span class="hid">filter.data.ptr</span>,
<a id="L496" href="#L496" class="br">496 </a>                                             <span class="hid">cudnnConvDesc</span>,
<a id="L497" href="#L497" class="br">497 </a>                                             <span class="hid">algo</span>,
<a id="L498" href="#L498" class="br">498 </a>                                             <span class="kwrd">cast</span>(<span class="type">void</span>*) <span class="hid">workSpace.ptr</span>, <span class="hid">workSpaceSize</span>,
<a id="L499" href="#L499" class="br">499 </a>                                             <span class="kwrd">cast</span>(<span class="kwrd">const</span> <span class="type">void</span>*) &amp;<span class="hid">beta</span>,
<a id="L500" href="#L500" class="br">500 </a>                                             <span class="hid">cudnnOdesc</span>, <span class="kwrd">cast</span>(<span class="type">void</span>*) <span class="hid">output.data.ptr</span>) );
<a id="L501" href="#L501" class="br">501 </a>}
<a id="L502" href="#L502" class="br">502 </a>
<a id="L503" href="#L503" class="br">503 </a><span class="com">/// wrapper of cudnnConvolutionBackwardData and Weight for Variable</span>
<a id="L504" href="#L504" class="br">504 </a><span class="type">void</span> <a href="grain.cudnn.d.html#L504" title="grain.cudnn.convBackward" class="hid">convBackward</a>(<span class="type">bool</span> <span class="hid">isConv</span>, <span class="type">bool</span> <span class="hid">isNchw</span>, <span class="hid">T</span>, <span class="hid">size_t</span> <span class="hid">dim</span>, <span class="hid">size_t</span> <span class="hid">imDims</span>
<a id="L505" href="#L505" class="br">505 </a>    )
<a id="L506" href="#L506" class="br">506 </a>    (
<a id="L507" href="#L507" class="br">507 </a>     <span class="kwrd">ref</span> <span class="hid">Variable</span>!(<span class="hid">T</span>, <span class="hid">dim</span>, <span class="hid">DeviceStorage</span>) <span class="hid">gradInput</span>,      <span class="com">// [N, CI, HI, WI]</span>
<a id="L508" href="#L508" class="br">508 </a>     <span class="hid">Variable</span>!(<span class="hid">T</span>, <span class="hid">dim</span>, <span class="hid">DeviceStorage</span>) <span class="hid">input</span>,      <span class="com">// [N, CI, HI, WI]</span>
<a id="L509" href="#L509" class="br">509 </a>     <span class="kwrd">ref</span> <span class="hid">Variable</span>!(<span class="hid">T</span>, <span class="hid">dim</span>, <span class="hid">DeviceStorage</span>) <span class="hid">gradFilter</span>,     <span class="com">// [CO, CI/G, KH, KW]</span>
<a id="L510" href="#L510" class="br">510 </a>     <span class="hid">Variable</span>!(<span class="hid">T</span>, <span class="hid">dim</span>, <span class="hid">DeviceStorage</span>) <span class="hid">filter</span>,     <span class="com">// [CO, CI/G, KH, KW]</span>
<a id="L511" href="#L511" class="br">511 </a>     <span class="hid">Variable</span>!(<span class="hid">T</span>, <span class="hid">dim</span>, <span class="hid">DeviceStorage</span>) <span class="hid">gradOutput</span>, <span class="com">// [N, CO, HO, WO]</span>
<a id="L512" href="#L512" class="br">512 </a>     <span class="kwrd">const</span> <span class="type">int</span>[<span class="hid">imDims</span>]   <span class="hid">stride</span>,
<a id="L513" href="#L513" class="br">513 </a>     <span class="kwrd">const</span> <span class="type">int</span>[<span class="hid">imDims</span>]   <span class="hid">pad</span>,
<a id="L514" href="#L514" class="br">514 </a>     <span class="kwrd">const</span> <span class="type">int</span>[<span class="hid">imDims</span>]   <span class="hid">dilation</span>,
<a id="L515" href="#L515" class="br">515 </a>     <span class="type">int</span> <span class="hid">ngroup</span> = <span class="num">1</span>,
<a id="L516" href="#L516" class="br">516 </a>     <span class="hid">cudnnConvolutionBwdDataAlgo_t</span> <span class="hid">algo</span> = <span class="hid">CUDNN_CONVOLUTION_BWD_DATA_ALGO_1</span>,
<a id="L517" href="#L517" class="br">517 </a>     <span class="type">float</span> <span class="hid">alpha</span> = <span class="num">1</span>,
<a id="L518" href="#L518" class="br">518 </a>     <span class="type">float</span> <span class="hid">beta</span> = <span class="num">0</span>
<a id="L519" href="#L519" class="br">519 </a>)  {
<a id="L520" href="#L520" class="br">520 </a>    <span class="kwrd">static</span> <span class="kwrd">assert</span>(<span class="hid">dim</span> &lt; <span class="hid">CUDNN_DIM_MAX</span>);
<a id="L521" href="#L521" class="br">521 </a>    <span class="kwrd">static</span> <span class="kwrd">assert</span>(<span class="hid">dim</span> == <span class="hid">imDims</span> + <span class="num">2</span>, <span class="str">&quot;dim should be like N(batch), C(channel) ~ dim(stride)&quot;</span>);
<a id="L522" href="#L522" class="br">522 </a>    <span class="kwrd">enum</span> <span class="hid">cudnnConvolutionMode_t</span> <span class="hid">mode</span> = <span class="hid">isConv</span> ? <span class="hid">CUDNN_CONVOLUTION</span> : <span class="hid">CUDNN_CROSS_CORRELATION</span>;
<a id="L523" href="#L523" class="br">523 </a>    <span class="kwrd">enum</span> <span class="hid">cudnnTensorFormat_t</span> <span class="hid">format</span> = <span class="hid">isNchw</span> ? <span class="hid">CUDNN_TENSOR_NCHW</span> : <span class="hid">CUDNN_TENSOR_NCHW</span>;
<a id="L524" href="#L524" class="br">524 </a>
<a id="L525" href="#L525" class="br">525 </a>    <span class="kwrd">static</span> <span class="kwrd">if</span> (<span class="hid">imDims</span> == <span class="num">1</span>) {
<a id="L526" href="#L526" class="br">526 </a>        <span class="kwrd">enum</span> <span class="hid">nbDim_</span> = <span class="num">4</span>;
<a id="L527" href="#L527" class="br">527 </a>        <span class="kwrd">enum</span> <span class="hid">imDim_</span> = <span class="num">2</span>;
<a id="L528" href="#L528" class="br">528 </a>        <span class="kwrd">const</span> <span class="hid">stride_</span> = <span class="hid">stride</span> ~ [<span class="num">1</span>];
<a id="L529" href="#L529" class="br">529 </a>        <span class="kwrd">const</span> <span class="hid">pad_</span> = <span class="hid">pad</span> ~ [<span class="num">0</span>];
<a id="L530" href="#L530" class="br">530 </a>        <span class="kwrd">const</span> <span class="hid">dilation_</span> = <span class="hid">dilation</span> ~ [<span class="num">1</span>];
<a id="L531" href="#L531" class="br">531 </a>        <span class="kwrd">const</span> <span class="hid">fshape_</span> = <span class="hid">filter.shape.castArray</span>!<span class="type">int</span> ~ [<span class="num">1</span>];
<a id="L532" href="#L532" class="br">532 </a>    } <span class="kwrd">else</span> {
<a id="L533" href="#L533" class="br">533 </a>        <span class="kwrd">enum</span> <span class="hid">nbDim_</span> = <span class="hid">dim</span>;
<a id="L534" href="#L534" class="br">534 </a>        <span class="kwrd">enum</span> <span class="hid">imDim_</span> = <span class="hid">imDims</span>;
<a id="L535" href="#L535" class="br">535 </a>        <span class="kwrd">const</span> <span class="hid">pad_</span> = <span class="hid">pad</span>;
<a id="L536" href="#L536" class="br">536 </a>        <span class="kwrd">const</span> <span class="hid">stride_</span> = <span class="hid">stride</span>;
<a id="L537" href="#L537" class="br">537 </a>        <span class="kwrd">const</span> <span class="hid">dilation_</span> = <span class="hid">dilation</span>;
<a id="L538" href="#L538" class="br">538 </a>        <span class="kwrd">const</span> <span class="hid">fshape_</span> = <span class="hid">filter.shape.castArray</span>!<span class="type">int</span>;
<a id="L539" href="#L539" class="br">539 </a>    }
<a id="L540" href="#L540" class="br">540 </a>
<a id="L541" href="#L541" class="br">541 </a>    <span class="com">// TODO cache these?</span>
<a id="L542" href="#L542" class="br">542 </a>    <span class="hid">cudnnFilterDescriptor_t</span> <span class="hid">cudnnFdesc</span>;
<a id="L543" href="#L543" class="br">543 </a>    <span class="hid">checkCUDNN</span>( <span class="hid">cudnnCreateFilterDescriptor</span>(&amp;<span class="hid">cudnnFdesc</span>) );
<a id="L544" href="#L544" class="br">544 </a>    <span class="kwrd">scope</span>(<span class="hid">exit</span>) <span class="hid">cudnnDestroyFilterDescriptor</span>(<span class="hid">cudnnFdesc</span>);
<a id="L545" href="#L545" class="br">545 </a>    <span class="hid">checkCUDNN</span>( <span class="hid">cudnnSetFilterNdDescriptor</span>(<span class="hid">cudnnFdesc</span>, <a href="grain.cudnn.d.html#L32" title="grain.cudnn.cudnnDataType" class="hid">cudnnDataType</a>!<span class="hid">T</span>, <span class="hid">format</span>,
<a id="L546" href="#L546" class="br">546 </a>                                           <span class="kwrd">cast</span>(<span class="type">int</span>) <span class="hid">nbDim_</span>, <span class="hid">fshape_.ptr</span>
<a id="L547" href="#L547" class="br">547 </a>                                           ) );
<a id="L548" href="#L548" class="br">548 </a>
<a id="L549" href="#L549" class="br">549 </a>    <span class="hid">cudnnConvolutionDescriptor_t</span> <span class="hid">cudnnConvDesc</span>;
<a id="L550" href="#L550" class="br">550 </a>    <span class="hid">checkCUDNN</span>( <span class="hid">cudnnCreateConvolutionDescriptor</span>(&amp;<span class="hid">cudnnConvDesc</span>) );
<a id="L551" href="#L551" class="br">551 </a>    <span class="kwrd">scope</span>(<span class="hid">exit</span>) <span class="hid">cudnnDestroyConvolutionDescriptor</span>(<span class="hid">cudnnConvDesc</span>);
<a id="L552" href="#L552" class="br">552 </a>    <span class="hid">checkCUDNN</span>( <span class="hid">cudnnSetConvolutionGroupCount</span>(<span class="hid">cudnnConvDesc</span>, <span class="hid">ngroup</span>) );
<a id="L553" href="#L553" class="br">553 </a>    <span class="hid">checkCUDNN</span>( <span class="hid">cudnnSetConvolutionNdDescriptor</span>(<span class="hid">cudnnConvDesc</span>, <span class="kwrd">cast</span>(<span class="type">int</span>) <span class="hid">imDim_</span>,
<a id="L554" href="#L554" class="br">554 </a>                                                <span class="hid">pad_.ptr</span>, <span class="hid">stride_.ptr</span>, <span class="hid">dilation_.ptr</span>,
<a id="L555" href="#L555" class="br">555 </a>                                                <span class="hid">mode</span>, <a href="grain.cudnn.d.html#L32" title="grain.cudnn.cudnnDataType" class="hid">cudnnDataType</a>!<span class="hid">T</span>
<a id="L556" href="#L556" class="br">556 </a>                                                ) );
<a id="L557" href="#L557" class="br">557 </a>
<a id="L558" href="#L558" class="br">558 </a>    <span class="kwrd">auto</span> <span class="hid">cudnnIdesc</span> = <span class="hid">input.makeCudnnTensor</span>;
<a id="L559" href="#L559" class="br">559 </a>    <span class="kwrd">auto</span> <span class="hid">cudnnGIdesc</span> = <span class="hid">gradInput.makeCudnnTensor</span>;
<a id="L560" href="#L560" class="br">560 </a>    <span class="kwrd">auto</span> <span class="hid">cudnnGOdesc</span> = <span class="hid">gradOutput.makeCudnnTensor</span>;
<a id="L561" href="#L561" class="br">561 </a>
<a id="L562" href="#L562" class="br">562 </a>    <span class="hid">size_t</span> <span class="hid">dworkSpaceSize</span>;
<a id="L563" href="#L563" class="br">563 </a>    <span class="hid">checkCUDNN</span> ( <span class="hid">cudnnGetConvolutionBackwardDataWorkspaceSize</span>
<a id="L564" href="#L564" class="br">564 </a>                    (<span class="hid">cudnnHandle</span>, <span class="hid">cudnnFdesc</span>, <span class="hid">cudnnGOdesc</span>, <span class="hid">cudnnConvDesc</span>,
<a id="L565" href="#L565" class="br">565 </a>                     <span class="hid">cudnnGIdesc</span>, <span class="hid">algo</span>, &amp;<span class="hid">dworkSpaceSize</span>) );
<a id="L566" href="#L566" class="br">566 </a>    <span class="kwrd">auto</span> <span class="hid">dworkSpace</span> = <span class="hid">CuPtr</span>!<span class="type">byte</span>(<span class="hid">dworkSpaceSize</span>);
<a id="L567" href="#L567" class="br">567 </a>    <span class="hid">checkCUDNN</span> ( <span class="hid">cudnnConvolutionBackwardData</span> (<span class="hid">cudnnHandle</span>,
<a id="L568" href="#L568" class="br">568 </a>                                                  <span class="kwrd">cast</span>(<span class="kwrd">const</span> <span class="type">void</span>*)(&amp;<span class="hid">alpha</span>),
<a id="L569" href="#L569" class="br">569 </a>                                                  <span class="hid">cudnnFdesc</span>, <span class="kwrd">cast</span>(<span class="kwrd">const</span> <span class="type">void</span>*) <span class="hid">filter.data.ptr</span>,
<a id="L570" href="#L570" class="br">570 </a>                                                  <span class="hid">cudnnGOdesc</span>, <span class="kwrd">cast</span>(<span class="kwrd">const</span> <span class="type">void</span>*) <span class="hid">gradOutput.data.ptr</span>,
<a id="L571" href="#L571" class="br">571 </a>                                                  <span class="hid">cudnnConvDesc</span>,
<a id="L572" href="#L572" class="br">572 </a>                                                  <span class="hid">algo</span>,
<a id="L573" href="#L573" class="br">573 </a>                                                  <span class="kwrd">cast</span>(<span class="type">void</span>*) <span class="hid">dworkSpace.ptr</span>, <span class="hid">dworkSpaceSize</span>,
<a id="L574" href="#L574" class="br">574 </a>                                                  <span class="kwrd">cast</span>(<span class="kwrd">const</span> <span class="type">void</span>*)(&amp;<span class="hid">beta</span>),
<a id="L575" href="#L575" class="br">575 </a>                                                  <span class="hid">cudnnGIdesc</span>, <span class="kwrd">cast</span>(<span class="type">void</span>*) <span class="hid">gradInput.data.ptr</span>) );
<a id="L576" href="#L576" class="br">576 </a>
<a id="L577" href="#L577" class="br">577 </a>    <span class="hid">size_t</span> <span class="hid">fworkSpaceSize</span>;
<a id="L578" href="#L578" class="br">578 </a>    <span class="hid">checkCUDNN</span> ( <span class="hid">cudnnGetConvolutionBackwardFilterWorkspaceSize</span>
<a id="L579" href="#L579" class="br">579 </a>                    (<span class="hid">cudnnHandle</span>, <span class="hid">cudnnIdesc</span>, <span class="hid">cudnnGOdesc</span>, <span class="hid">cudnnConvDesc</span>,
<a id="L580" href="#L580" class="br">580 </a>                     <span class="hid">cudnnFdesc</span>, <span class="hid">algo</span>, &amp;<span class="hid">fworkSpaceSize</span>) );
<a id="L581" href="#L581" class="br">581 </a>    <span class="kwrd">auto</span> <span class="hid">fworkSpace</span> = <span class="hid">CuPtr</span>!<span class="type">byte</span>(<span class="hid">fworkSpaceSize</span>);
<a id="L582" href="#L582" class="br">582 </a>    <span class="hid">checkCUDNN</span> ( <span class="hid">cudnnConvolutionBackwardFilter</span> (<span class="hid">cudnnHandle</span>,
<a id="L583" href="#L583" class="br">583 </a>                                                    <span class="kwrd">cast</span>(<span class="kwrd">const</span> <span class="type">void</span>*)(&amp;<span class="hid">alpha</span>),
<a id="L584" href="#L584" class="br">584 </a>                                                    <span class="hid">cudnnIdesc</span>,  <span class="kwrd">cast</span>(<span class="kwrd">const</span> <span class="type">void</span>*) <span class="hid">input.data.ptr</span>,
<a id="L585" href="#L585" class="br">585 </a>                                                    <span class="hid">cudnnGOdesc</span>, <span class="kwrd">cast</span>(<span class="kwrd">const</span> <span class="type">void</span>*) <span class="hid">gradOutput.data.ptr</span>,
<a id="L586" href="#L586" class="br">586 </a>                                                    <span class="hid">cudnnConvDesc</span>,
<a id="L587" href="#L587" class="br">587 </a>                                                    <span class="hid">algo</span>,
<a id="L588" href="#L588" class="br">588 </a>                                                    <span class="kwrd">cast</span>(<span class="type">void</span>*) <span class="hid">fworkSpace.ptr</span>, <span class="hid">fworkSpaceSize</span>,
<a id="L589" href="#L589" class="br">589 </a>                                                    <span class="kwrd">cast</span>(<span class="kwrd">const</span> <span class="type">void</span>*)(&amp;<span class="hid">beta</span>),
<a id="L590" href="#L590" class="br">590 </a>                                                    <span class="hid">cudnnFdesc</span>, <span class="kwrd">cast</span>(<span class="type">void</span>*) <span class="hid">gradFilter.data.ptr</span>) );
<a id="L591" href="#L591" class="br">591 </a>}
<a id="L592" href="#L592" class="br">592 </a>
<a id="L593" href="#L593" class="br">593 </a>
<a id="L594" href="#L594" class="br">594 </a><span class="com">/// wrapper of cudnnPoolingForward for Variable</span>
<a id="L595" href="#L595" class="br">595 </a><span class="kwrd">auto</span> <a href="grain.cudnn.d.html#L595" title="grain.cudnn.poolForward" class="hid">poolForward</a>(<span class="type">bool</span> <span class="hid">isMax</span> = <span class="kwrd">true</span>, <span class="type">bool</span> <span class="hid">isAveragePad</span> = <span class="kwrd">false</span>,
<a id="L596" href="#L596" class="br">596 </a>                 <span class="hid">T</span>, <span class="hid">size_t</span> <span class="hid">_tensorDims</span>, <span class="hid">size_t</span> <span class="hid">_poolDims</span>)
<a id="L597" href="#L597" class="br">597 </a>    (<span class="hid">Variable</span>!(<span class="hid">T</span>, <span class="hid">_tensorDims</span>, <span class="hid">DeviceStorage</span>) <span class="hid">input</span>,      <span class="com">// [N, C, HI, WI]</span>
<a id="L598" href="#L598" class="br">598 </a>     <span class="type">int</span>[<span class="hid">_poolDims</span>] <span class="hid">windowDim</span>,
<a id="L599" href="#L599" class="br">599 </a>     <span class="type">int</span>[<span class="hid">_poolDims</span>] <span class="hid">padding</span>,
<a id="L600" href="#L600" class="br">600 </a>     <span class="type">int</span>[<span class="hid">_poolDims</span>] <span class="hid">stride</span>,
<a id="L601" href="#L601" class="br">601 </a>     <span class="hid">T</span> <span class="hid">alpha</span> = <span class="num">1</span>,
<a id="L602" href="#L602" class="br">602 </a>     <span class="hid">T</span> <span class="hid">beta</span> = <span class="num">0</span>
<a id="L603" href="#L603" class="br">603 </a>        ) {
<a id="L604" href="#L604" class="br">604 </a>    <span class="kwrd">static</span> <span class="kwrd">assert</span>(<span class="hid">_tensorDims</span> &lt; <span class="hid">CUDNN_DIM_MAX</span>);
<a id="L605" href="#L605" class="br">605 </a>    <span class="kwrd">static</span> <span class="kwrd">assert</span>(<span class="hid">_tensorDims</span> == <span class="hid">_poolDims</span> + <span class="num">2</span>);
<a id="L606" href="#L606" class="br">606 </a>
<a id="L607" href="#L607" class="br">607 </a>    <span class="kwrd">static</span> <span class="kwrd">if</span> (<span class="hid">_poolDims</span> == <span class="num">1</span>) {
<a id="L608" href="#L608" class="br">608 </a>        <span class="kwrd">enum</span> <span class="hid">tensorDims</span> = <span class="num">4</span>;
<a id="L609" href="#L609" class="br">609 </a>        <span class="kwrd">enum</span> <span class="hid">poolDims</span> = <span class="num">2</span>;
<a id="L610" href="#L610" class="br">610 </a>        <span class="kwrd">const</span> <span class="hid">strideA</span> = <span class="hid">stride</span> ~ [<span class="num">1</span>];
<a id="L611" href="#L611" class="br">611 </a>        <span class="kwrd">const</span> <span class="hid">paddingA</span> = <span class="hid">padding</span> ~ [<span class="num">0</span>];
<a id="L612" href="#L612" class="br">612 </a>        <span class="kwrd">const</span> <span class="hid">windowDimA</span> = <span class="hid">windowDim</span> ~ [<span class="num">1</span>];
<a id="L613" href="#L613" class="br">613 </a>    } <span class="kwrd">else</span> {
<a id="L614" href="#L614" class="br">614 </a>        <span class="kwrd">enum</span> <span class="hid">tensorDims</span> = <span class="hid">_tensorDims</span>;
<a id="L615" href="#L615" class="br">615 </a>        <span class="kwrd">enum</span> <span class="hid">poolDims</span> = <span class="hid">_poolDims</span>;
<a id="L616" href="#L616" class="br">616 </a>        <span class="kwrd">const</span> <span class="hid">strideA</span> = <span class="hid">stride</span>;
<a id="L617" href="#L617" class="br">617 </a>        <span class="kwrd">const</span> <span class="hid">paddingA</span> = <span class="hid">padding</span>;
<a id="L618" href="#L618" class="br">618 </a>        <span class="kwrd">const</span> <span class="hid">windowDimA</span> = <span class="hid">windowDim</span>;
<a id="L619" href="#L619" class="br">619 </a>    }
<a id="L620" href="#L620" class="br">620 </a>
<a id="L621" href="#L621" class="br">621 </a>    <span class="hid">cudnnPoolingDescriptor_t</span>     <span class="hid">poolingDesc</span>;
<a id="L622" href="#L622" class="br">622 </a>    <span class="hid">checkCUDNN</span>( <span class="hid">cudnnCreatePoolingDescriptor</span>(&amp;<span class="hid">poolingDesc</span>) );
<a id="L623" href="#L623" class="br">623 </a>    <span class="kwrd">scope</span>(<span class="hid">exit</span>) <span class="hid">checkCUDNN</span>( <span class="hid">cudnnDestroyPoolingDescriptor</span>(<span class="hid">poolingDesc</span>) );
<a id="L624" href="#L624" class="br">624 </a>
<a id="L625" href="#L625" class="br">625 </a>    <span class="kwrd">static</span> <span class="kwrd">if</span> (<span class="hid">isMax</span>) {
<a id="L626" href="#L626" class="br">626 </a>        <span class="kwrd">immutable</span> <span class="hid">mode</span> = <a href="grain.cudnn.d.html#L21" title="grain.cudnn.isDeterministic" class="hid">isDeterministic</a>() == <span class="hid">CUDNN_DETERMINISTIC</span>
<a id="L627" href="#L627" class="br">627 </a>            ? <span class="hid">CUDNN_POOLING_MAX_DETERMINISTIC</span>
<a id="L628" href="#L628" class="br">628 </a>            : <span class="hid">CUDNN_POOLING_MAX</span>;
<a id="L629" href="#L629" class="br">629 </a>    } <span class="kwrd">else</span> {
<a id="L630" href="#L630" class="br">630 </a>        <span class="kwrd">enum</span> <span class="hid">mode</span> = <span class="hid">isAveragePad</span>
<a id="L631" href="#L631" class="br">631 </a>            ? <span class="hid">CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING</span>
<a id="L632" href="#L632" class="br">632 </a>            : <span class="hid">CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING</span>;
<a id="L633" href="#L633" class="br">633 </a>    }
<a id="L634" href="#L634" class="br">634 </a>    <span class="hid">checkCUDNN</span>( <span class="hid">cudnnSetPoolingNdDescriptor</span>(<span class="hid">poolingDesc</span>,
<a id="L635" href="#L635" class="br">635 </a>                                            <span class="hid">mode</span>,
<a id="L636" href="#L636" class="br">636 </a>                                            <a href="grain.cudnn.d.html#L26" title="grain.cudnn.isNanProp" class="hid">isNanProp</a>(),
<a id="L637" href="#L637" class="br">637 </a>                                            <span class="kwrd">cast</span>(<span class="type">int</span>) <span class="hid">poolDims</span>,
<a id="L638" href="#L638" class="br">638 </a>                                            <span class="hid">windowDimA.ptr</span>,
<a id="L639" href="#L639" class="br">639 </a>                                            <span class="hid">paddingA.ptr</span>,
<a id="L640" href="#L640" class="br">640 </a>                                            <span class="hid">strideA.ptr</span> ) );
<a id="L641" href="#L641" class="br">641 </a>
<a id="L642" href="#L642" class="br">642 </a>    <span class="kwrd">const</span> <span class="hid">inputDesc</span> = <span class="hid">input.makeCudnnTensor</span>;
<a id="L643" href="#L643" class="br">643 </a>    <span class="type">int</span>[<span class="hid">tensorDims</span>] <span class="hid">tensorOutputDimA</span>;
<a id="L644" href="#L644" class="br">644 </a>    <span class="hid">checkCUDNN</span>( <span class="hid">cudnnGetPoolingNdForwardOutputDim</span>(<span class="hid">poolingDesc</span>,
<a id="L645" href="#L645" class="br">645 </a>                                                  <span class="hid">inputDesc</span>,
<a id="L646" href="#L646" class="br">646 </a>                                                  <span class="kwrd">cast</span>(<span class="type">int</span>) <span class="hid">tensorDims</span>,
<a id="L647" href="#L647" class="br">647 </a>                                                  <span class="hid">tensorOutputDimA.ptr</span>) );
<a id="L648" href="#L648" class="br">648 </a>    <span class="com">// resize output if shape is not met</span>
<a id="L649" href="#L649" class="br">649 </a>    <span class="com">// if (tensorOutputDimA != output.shape.castArray!int) {</span>
<a id="L650" href="#L650" class="br">650 </a>    <span class="kwrd">auto</span> <span class="hid">output</span> = <span class="hid">uninitVariable</span>!(<span class="hid">T</span>, <span class="hid">DeviceStorage</span>, <span class="hid">tensorDims</span>)(<span class="hid">tensorOutputDimA.castArray</span>!<span class="type">uint</span>, <span class="hid">input.requiresGrad</span>);
<a id="L651" href="#L651" class="br">651 </a>
<a id="L652" href="#L652" class="br">652 </a>    <span class="hid">checkCUDNN</span>( <span class="hid">cudnnPoolingForward</span>(<span class="hid">cudnnHandle</span>,
<a id="L653" href="#L653" class="br">653 </a>                                    <span class="hid">poolingDesc</span>,
<a id="L654" href="#L654" class="br">654 </a>                                    <span class="kwrd">cast</span>(<span class="kwrd">const</span> <span class="type">void</span>*) &amp;<span class="hid">alpha</span>,
<a id="L655" href="#L655" class="br">655 </a>                                    <span class="hid">inputDesc</span>,
<a id="L656" href="#L656" class="br">656 </a>                                    <span class="kwrd">cast</span>(<span class="kwrd">const</span> <span class="type">void</span>*) <span class="hid">input.data.ptr</span>,
<a id="L657" href="#L657" class="br">657 </a>                                    <span class="kwrd">cast</span>(<span class="kwrd">const</span> <span class="type">void</span>*) &amp;<span class="hid">beta</span>,
<a id="L658" href="#L658" class="br">658 </a>                                    <span class="hid">output.makeCudnnTensor</span>,
<a id="L659" href="#L659" class="br">659 </a>                                    <span class="kwrd">cast</span>(<span class="type">void</span>*) <span class="hid">output.data.ptr</span>) );
<a id="L660" href="#L660" class="br">660 </a>    <span class="kwrd">return</span> <span class="hid">output</span>;
<a id="L661" href="#L661" class="br">661 </a>}
<a id="L662" href="#L662" class="br">662 </a>
<a id="L663" href="#L663" class="br">663 </a>
<a id="L664" href="#L664" class="br">664 </a><span class="com">/// wrapper of cudnnPoolingBackward for Variable</span>
<a id="L665" href="#L665" class="br">665 </a><span class="type">void</span> <a href="grain.cudnn.d.html#L665" title="grain.cudnn.poolBackward" class="hid">poolBackward</a>(<span class="type">bool</span> <span class="hid">isMax</span> = <span class="kwrd">true</span>, <span class="type">bool</span> <span class="hid">isAveragePad</span> = <span class="kwrd">false</span>,
<a id="L666" href="#L666" class="br">666 </a>                  <span class="hid">T</span>, <span class="hid">size_t</span> <span class="hid">_tensorDims</span>, <span class="hid">size_t</span> <span class="hid">_poolDims</span>)
<a id="L667" href="#L667" class="br">667 </a>    (<span class="kwrd">ref</span> <span class="hid">Variable</span>!(<span class="hid">T</span>, <span class="hid">_tensorDims</span>, <span class="hid">DeviceStorage</span>) <span class="hid">gradInput</span>,
<a id="L668" href="#L668" class="br">668 </a>     <span class="hid">Variable</span>!(<span class="hid">T</span>, <span class="hid">_tensorDims</span>, <span class="hid">DeviceStorage</span>) <span class="hid">input</span>,
<a id="L669" href="#L669" class="br">669 </a>     <span class="hid">Variable</span>!(<span class="hid">T</span>, <span class="hid">_tensorDims</span>, <span class="hid">DeviceStorage</span>) <span class="hid">gradOutput</span>,
<a id="L670" href="#L670" class="br">670 </a>     <span class="hid">Variable</span>!(<span class="hid">T</span>, <span class="hid">_tensorDims</span>, <span class="hid">DeviceStorage</span>) <span class="hid">output</span>,
<a id="L671" href="#L671" class="br">671 </a>     <span class="type">int</span>[<span class="hid">_poolDims</span>] <span class="hid">windowDim</span>,
<a id="L672" href="#L672" class="br">672 </a>     <span class="type">int</span>[<span class="hid">_poolDims</span>] <span class="hid">padding</span>,
<a id="L673" href="#L673" class="br">673 </a>     <span class="type">int</span>[<span class="hid">_poolDims</span>] <span class="hid">stride</span>,
<a id="L674" href="#L674" class="br">674 </a>     <span class="hid">T</span> <span class="hid">alpha</span> = <span class="num">1</span>,
<a id="L675" href="#L675" class="br">675 </a>     <span class="hid">T</span> <span class="hid">beta</span> = <span class="num">0</span>
<a id="L676" href="#L676" class="br">676 </a>        ) {
<a id="L677" href="#L677" class="br">677 </a>    <span class="kwrd">static</span> <span class="kwrd">assert</span>(<span class="hid">_tensorDims</span> &lt; <span class="hid">CUDNN_DIM_MAX</span>);
<a id="L678" href="#L678" class="br">678 </a>    <span class="kwrd">static</span> <span class="kwrd">assert</span>(<span class="hid">_tensorDims</span> == <span class="hid">_poolDims</span> + <span class="num">2</span>);
<a id="L679" href="#L679" class="br">679 </a>
<a id="L680" href="#L680" class="br">680 </a>    <span class="kwrd">static</span> <span class="kwrd">if</span> (<span class="hid">_poolDims</span> == <span class="num">1</span>) {
<a id="L681" href="#L681" class="br">681 </a>        <span class="kwrd">enum</span> <span class="hid">tensorDims</span> = <span class="num">4</span>;
<a id="L682" href="#L682" class="br">682 </a>        <span class="kwrd">enum</span> <span class="hid">poolDims</span> = <span class="num">2</span>;
<a id="L683" href="#L683" class="br">683 </a>        <span class="kwrd">const</span> <span class="hid">strideA</span> = <span class="hid">stride</span> ~ [<span class="num">1</span>];
<a id="L684" href="#L684" class="br">684 </a>        <span class="kwrd">const</span> <span class="hid">paddingA</span> = <span class="hid">padding</span> ~ [<span class="num">0</span>];
<a id="L685" href="#L685" class="br">685 </a>        <span class="kwrd">const</span> <span class="hid">windowDimA</span> = <span class="hid">windowDim</span> ~ [<span class="num">1</span>];
<a id="L686" href="#L686" class="br">686 </a>    } <span class="kwrd">else</span> {
<a id="L687" href="#L687" class="br">687 </a>        <span class="kwrd">enum</span> <span class="hid">tensorDims</span> = <span class="hid">_tensorDims</span>;
<a id="L688" href="#L688" class="br">688 </a>        <span class="kwrd">enum</span> <span class="hid">poolDims</span> = <span class="hid">_poolDims</span>;
<a id="L689" href="#L689" class="br">689 </a>        <span class="kwrd">const</span> <span class="hid">strideA</span> = <span class="hid">stride</span>;
<a id="L690" href="#L690" class="br">690 </a>        <span class="kwrd">const</span> <span class="hid">paddingA</span> = <span class="hid">padding</span>;
<a id="L691" href="#L691" class="br">691 </a>        <span class="kwrd">const</span> <span class="hid">windowDimA</span> = <span class="hid">windowDim</span>;
<a id="L692" href="#L692" class="br">692 </a>    }
<a id="L693" href="#L693" class="br">693 </a>
<a id="L694" href="#L694" class="br">694 </a>    <span class="hid">cudnnPoolingDescriptor_t</span>     <span class="hid">poolingDesc</span>;
<a id="L695" href="#L695" class="br">695 </a>    <span class="hid">checkCUDNN</span>( <span class="hid">cudnnCreatePoolingDescriptor</span>(&amp;<span class="hid">poolingDesc</span>) );
<a id="L696" href="#L696" class="br">696 </a>    <span class="kwrd">scope</span>(<span class="hid">exit</span>) <span class="hid">checkCUDNN</span>( <span class="hid">cudnnDestroyPoolingDescriptor</span>(<span class="hid">poolingDesc</span>) );
<a id="L697" href="#L697" class="br">697 </a>
<a id="L698" href="#L698" class="br">698 </a>    <span class="kwrd">static</span> <span class="kwrd">if</span> (<span class="hid">isMax</span>) {
<a id="L699" href="#L699" class="br">699 </a>        <span class="kwrd">immutable</span> <span class="hid">mode</span> = <a href="grain.cudnn.d.html#L21" title="grain.cudnn.isDeterministic" class="hid">isDeterministic</a>() == <span class="hid">CUDNN_DETERMINISTIC</span>
<a id="L700" href="#L700" class="br">700 </a>            ? <span class="hid">CUDNN_POOLING_MAX_DETERMINISTIC</span>
<a id="L701" href="#L701" class="br">701 </a>            : <span class="hid">CUDNN_POOLING_MAX</span>;
<a id="L702" href="#L702" class="br">702 </a>    } <span class="kwrd">else</span> {
<a id="L703" href="#L703" class="br">703 </a>        <span class="kwrd">enum</span> <span class="hid">mode</span> = <span class="hid">isAveragePad</span>
<a id="L704" href="#L704" class="br">704 </a>            ? <span class="hid">CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING</span>
<a id="L705" href="#L705" class="br">705 </a>            : <span class="hid">CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING</span>;
<a id="L706" href="#L706" class="br">706 </a>    }
<a id="L707" href="#L707" class="br">707 </a>    <span class="hid">checkCUDNN</span>( <span class="hid">cudnnSetPoolingNdDescriptor</span>(<span class="hid">poolingDesc</span>,
<a id="L708" href="#L708" class="br">708 </a>                                            <span class="hid">mode</span>,
<a id="L709" href="#L709" class="br">709 </a>                                            <a href="grain.cudnn.d.html#L26" title="grain.cudnn.isNanProp" class="hid">isNanProp</a>(),
<a id="L710" href="#L710" class="br">710 </a>                                            <span class="kwrd">cast</span>(<span class="type">int</span>) <span class="hid">poolDims</span>,
<a id="L711" href="#L711" class="br">711 </a>                                            <span class="hid">windowDimA.ptr</span>,
<a id="L712" href="#L712" class="br">712 </a>                                            <span class="hid">paddingA.ptr</span>,
<a id="L713" href="#L713" class="br">713 </a>                                            <span class="hid">strideA.ptr</span> ) );
<a id="L714" href="#L714" class="br">714 </a>
<a id="L715" href="#L715" class="br">715 </a>    <span class="hid">checkCUDNN</span>( <span class="hid">cudnnPoolingBackward</span>(<span class="hid">cudnnHandle</span>,
<a id="L716" href="#L716" class="br">716 </a>                                     <span class="hid">poolingDesc</span>,
<a id="L717" href="#L717" class="br">717 </a>
<a id="L718" href="#L718" class="br">718 </a>                                     <span class="kwrd">cast</span>(<span class="kwrd">const</span> <span class="type">void</span>*) &amp;<span class="hid">alpha</span>,
<a id="L719" href="#L719" class="br">719 </a>                                     <span class="hid">output.makeCudnnTensor</span>,
<a id="L720" href="#L720" class="br">720 </a>                                     <span class="kwrd">cast</span>(<span class="kwrd">const</span> <span class="type">void</span>*) <span class="hid">output.data.ptr</span>,
<a id="L721" href="#L721" class="br">721 </a>                                     <span class="hid">gradOutput.makeCudnnTensor</span>,
<a id="L722" href="#L722" class="br">722 </a>                                     <span class="kwrd">cast</span>(<span class="kwrd">const</span> <span class="type">void</span>*) <span class="hid">gradOutput.data.ptr</span>,
<a id="L723" href="#L723" class="br">723 </a>                                     <span class="hid">input.makeCudnnTensor</span>,
<a id="L724" href="#L724" class="br">724 </a>                                     <span class="kwrd">cast</span>(<span class="kwrd">const</span> <span class="type">void</span>*) <span class="hid">input.data.ptr</span>,
<a id="L725" href="#L725" class="br">725 </a>
<a id="L726" href="#L726" class="br">726 </a>                                     <span class="kwrd">cast</span>(<span class="kwrd">const</span> <span class="type">void</span>*) &amp;<span class="hid">beta</span>,
<a id="L727" href="#L727" class="br">727 </a>                                     <span class="hid">gradInput.makeCudnnTensor</span>,
<a id="L728" href="#L728" class="br">728 </a>                                     <span class="kwrd">cast</span>(<span class="type">void</span>*) <span class="hid">gradInput.data.ptr</span>) );
<a id="L729" href="#L729" class="br">729 </a>}
<a id="L730" href="#L730" class="br">730 </a>
<a id="L731" href="#L731" class="br">731 </a>
<a id="L732" href="#L732" class="br">732 </a><span class="com">/// Global (thread local) dropout state with descriptor and state array</span>
<a id="L733" href="#L733" class="br">733 </a><span class="kwrd">struct</span> <a href="grain.cudnn.d.html#L733" title="grain.cudnn.ThreadLocalDropout" class="hid">ThreadLocalDropout</a> {
<a id="L734" href="#L734" class="br">734 </a>    <span class="kwrd">static</span> <span class="kwrd">shared</span> <span class="hid">size_t</span> <span class="hid">count</span> = <span class="num">0</span>;
<a id="L735" href="#L735" class="br">735 </a>    <span class="kwrd">static</span> <span class="hid">cudnnDropoutDescriptor_t</span> <span class="hid">_dropoutDesc</span> = <span class="kwrd">null</span>;
<a id="L736" href="#L736" class="br">736 </a>    <span class="kwrd">static</span> <span class="hid">CuPtr</span>!<span class="type">byte</span> <span class="hid">_stateArray</span>;
<a id="L737" href="#L737" class="br">737 </a>
<a id="L738" href="#L738" class="br">738 </a>    @<span class="hid">disable</span> <span class="kwrd">this</span>(<span class="kwrd">this</span>);
<a id="L739" href="#L739" class="br">739 </a>    @<span class="hid">disable</span> <span class="kwrd">new</span>(<span class="hid">size_t</span>);
<a id="L740" href="#L740" class="br">740 </a>
<a id="L741" href="#L741" class="br">741 </a>    <span class="com">/// FIXME set global seed</span>
<a id="L742" href="#L742" class="br">742 </a>    <span class="kwrd">static</span> <span class="type">void</span> <span class="hid">init</span>(<span class="hid">size_t</span> <span class="hid">seed</span>=<span class="num">0</span>) {
<a id="L743" href="#L743" class="br">743 </a>        <span class="kwrd">if</span> (<span class="hid">_dropoutDesc</span> != <span class="kwrd">null</span>) <span class="kwrd">return</span>;
<a id="L744" href="#L744" class="br">744 </a>
<a id="L745" href="#L745" class="br">745 </a>        <span class="hid">checkCUDNN</span>(<span class="hid">cudnnCreateDropoutDescriptor</span>(&amp;<span class="hid">_dropoutDesc</span>));
<a id="L746" href="#L746" class="br">746 </a>        <span class="com">// How much memory does dropout need for states?</span>
<a id="L747" href="#L747" class="br">747 </a>        <span class="com">// These states are used to generate random numbers internally</span>
<a id="L748" href="#L748" class="br">748 </a>        <span class="com">// and should not be freed until the RNN descriptor is no longer used</span>
<a id="L749" href="#L749" class="br">749 </a>        <span class="hid">size_t</span> <span class="hid">stateSize</span>;
<a id="L750" href="#L750" class="br">750 </a>        <span class="hid">checkCUDNN</span>(<span class="hid">cudnnDropoutGetStatesSize</span>(<span class="hid">cudnnHandle</span>, &amp;<span class="hid">stateSize</span>));
<a id="L751" href="#L751" class="br">751 </a>        <span class="hid">_stateArray</span> = <span class="hid">CuPtr</span>!<span class="type">byte</span>(<span class="hid">stateSize</span>);
<a id="L752" href="#L752" class="br">752 </a>        <span class="hid">checkCUDNN</span>(<span class="hid">cudnnSetDropoutDescriptor</span>(<span class="hid">_dropoutDesc</span>,
<a id="L753" href="#L753" class="br">753 </a>                                             <span class="hid">cudnnHandle</span>,
<a id="L754" href="#L754" class="br">754 </a>                                             <span class="num">0f</span>,
<a id="L755" href="#L755" class="br">755 </a>                                             <span class="kwrd">cast</span>(<span class="type">void</span>*) <span class="hid">_stateArray.ptr</span>,
<a id="L756" href="#L756" class="br">756 </a>                                             <span class="hid">stateSize</span>,
<a id="L757" href="#L757" class="br">757 </a>                                             <span class="hid">seed</span>));
<a id="L758" href="#L758" class="br">758 </a>
<a id="L759" href="#L759" class="br">759 </a>        <span class="kwrd">import</span> <span class="hid">core.atomic</span> : <span class="hid">atomicOp</span>;
<a id="L760" href="#L760" class="br">760 </a>        <span class="hid">count.atomicOp</span>!<span class="str">&quot;+=&quot;</span>(<span class="num">1</span>);
<a id="L761" href="#L761" class="br">761 </a>    }
<a id="L762" href="#L762" class="br">762 </a>
<a id="L763" href="#L763" class="br">763 </a>    <span class="kwrd">static</span> <span class="hid">descriptor</span>(<span class="type">float</span> <span class="hid">ratio</span>=<span class="num">0.0</span>) {
<a id="L764" href="#L764" class="br">764 </a>        <span class="hid">init</span>();
<a id="L765" href="#L765" class="br">765 </a>        <span class="kwrd">if</span> (<span class="hid">ratio</span> != <span class="num">0.0</span>) {
<a id="L766" href="#L766" class="br">766 </a>            <span class="hid">checkCUDNN</span>(<span class="hid">cudnnSetDropoutDescriptor</span>(
<a id="L767" href="#L767" class="br">767 </a>                           <span class="hid">_dropoutDesc</span>,
<a id="L768" href="#L768" class="br">768 </a>                           <span class="hid">cudnnHandle</span>,
<a id="L769" href="#L769" class="br">769 </a>                           <span class="hid">ratio</span>,
<a id="L770" href="#L770" class="br">770 </a>                           <span class="kwrd">null</span>, <span class="com">// if state is null, state won't be updated</span>
<a id="L771" href="#L771" class="br">771 </a>                           <span class="num">0</span>,
<a id="L772" href="#L772" class="br">772 </a>                           <span class="num">0</span>));
<a id="L773" href="#L773" class="br">773 </a>        }
<a id="L774" href="#L774" class="br">774 </a>        <span class="kwrd">return</span> <span class="kwrd">this</span>.<span class="hid">_dropoutDesc</span>;
<a id="L775" href="#L775" class="br">775 </a>    }
<a id="L776" href="#L776" class="br">776 </a>
<a id="L777" href="#L777" class="br">777 </a>    <span class="kwrd">static</span> <span class="kwrd">ref</span> <span class="hid">state</span>() {
<a id="L778" href="#L778" class="br">778 </a>        <span class="hid">init</span>();
<a id="L779" href="#L779" class="br">779 </a>        <span class="kwrd">return</span> <span class="hid">_stateArray</span>;
<a id="L780" href="#L780" class="br">780 </a>    }
<a id="L781" href="#L781" class="br">781 </a>}
<a id="L782" href="#L782" class="br">782 </a>
<a id="L783" href="#L783" class="br">783 </a><span class="kwrd">struct</span> <a href="grain.cudnn.d.html#L783" title="grain.cudnn.CudnnDropout" class="hid">CudnnDropout</a> {
<a id="L784" href="#L784" class="br">784 </a>    <span class="hid">CuPtr</span>!<span class="type">byte</span> <span class="hid">reserved</span>;
<a id="L785" href="#L785" class="br">785 </a>
<a id="L786" href="#L786" class="br">786 </a>    <span class="kwrd">auto</span> <span class="hid">forward</span>(<span class="hid">size_t</span> <span class="hid">dim</span>)(<span class="hid">Variable</span>!(<span class="type">float</span>, <span class="hid">dim</span>, <span class="hid">DeviceStorage</span>) <span class="hid">x</span>, <span class="type">float</span> <span class="hid">ratio</span>) {
<a id="L787" href="#L787" class="br">787 </a>        <span class="kwrd">import</span> <span class="hid">std.algorithm</span> : <span class="hid">move</span>;
<a id="L788" href="#L788" class="br">788 </a>        <span class="kwrd">auto</span> <span class="hid">y</span> = <span class="hid">x.uninit</span>;
<a id="L789" href="#L789" class="br">789 </a>        <span class="kwrd">auto</span> <span class="hid">xt</span> = <span class="hid">x.makeCudnnTensor</span>;
<a id="L790" href="#L790" class="br">790 </a>
<a id="L791" href="#L791" class="br">791 </a>        <span class="hid">size_t</span> <span class="hid">reservedSize</span>;
<a id="L792" href="#L792" class="br">792 </a>        <span class="hid">cudnnDropoutGetReserveSpaceSize</span>(<span class="hid">xt</span>, &amp;<span class="hid">reservedSize</span>);
<a id="L793" href="#L793" class="br">793 </a>        <span class="kwrd">this</span>.<span class="hid">reserved</span> = <span class="hid">CuPtr</span>!<span class="type">byte</span>(<span class="hid">reservedSize</span>);
<a id="L794" href="#L794" class="br">794 </a>
<a id="L795" href="#L795" class="br">795 </a>        <span class="hid">checkCUDNN</span>(<span class="hid">cudnnDropoutForward</span>(
<a id="L796" href="#L796" class="br">796 </a>                       <span class="hid">cudnnHandle</span>,
<a id="L797" href="#L797" class="br">797 </a>                       <a href="grain.cudnn.d.html#L763" title="grain.cudnn.ThreadLocalDropout.descriptor" class="hid">ThreadLocalDropout.descriptor</a>(<span class="hid">ratio</span>),
<a id="L798" href="#L798" class="br">798 </a>                       <span class="hid">xt</span>,
<a id="L799" href="#L799" class="br">799 </a>                       <span class="kwrd">cast</span>(<span class="type">void</span>*) <span class="hid">x.ptr</span>,
<a id="L800" href="#L800" class="br">800 </a>                       <span class="hid">y.makeCudnnTensor</span>,
<a id="L801" href="#L801" class="br">801 </a>                       <span class="kwrd">cast</span>(<span class="type">void</span>*) <span class="hid">y.ptr</span>,
<a id="L802" href="#L802" class="br">802 </a>                       <span class="kwrd">cast</span>(<span class="type">void</span>*) <span class="kwrd">this</span>.<span class="hid">reserved.ptr</span>,
<a id="L803" href="#L803" class="br">803 </a>                       <span class="kwrd">this</span>.<span class="hid">reserved.length</span>
<a id="L804" href="#L804" class="br">804 </a>                       ));
<a id="L805" href="#L805" class="br">805 </a>        <span class="kwrd">return</span> <span class="hid">y</span>;
<a id="L806" href="#L806" class="br">806 </a>    }
<a id="L807" href="#L807" class="br">807 </a>
<a id="L808" href="#L808" class="br">808 </a>    <span class="kwrd">auto</span> <span class="hid">backward</span>(<span class="hid">size_t</span> <span class="hid">dim</span>)(<span class="hid">Variable</span>!(<span class="type">float</span>, <span class="hid">dim</span>, <span class="hid">DeviceStorage</span>) <span class="hid">gy</span>) {
<a id="L809" href="#L809" class="br">809 </a>        <span class="kwrd">auto</span> <span class="hid">gx</span> = <span class="hid">gy.uninit</span>;
<a id="L810" href="#L810" class="br">810 </a>        <span class="hid">checkCUDNN</span>(<span class="hid">cudnnDropoutBackward</span>(
<a id="L811" href="#L811" class="br">811 </a>                       <span class="hid">cudnnHandle</span>,
<a id="L812" href="#L812" class="br">812 </a>                       <a href="grain.cudnn.d.html#L763" title="grain.cudnn.ThreadLocalDropout.descriptor" class="hid">ThreadLocalDropout.descriptor</a>,
<a id="L813" href="#L813" class="br">813 </a>                       <span class="hid">gy.makeCudnnTensor</span>,
<a id="L814" href="#L814" class="br">814 </a>                       <span class="kwrd">cast</span>(<span class="type">void</span>*) <span class="hid">gy.ptr</span>,
<a id="L815" href="#L815" class="br">815 </a>                       <span class="hid">gx.makeCudnnTensor</span>,
<a id="L816" href="#L816" class="br">816 </a>                       <span class="kwrd">cast</span>(<span class="type">void</span>*) <span class="hid">gx.ptr</span>,
<a id="L817" href="#L817" class="br">817 </a>                       <span class="kwrd">cast</span>(<span class="type">void</span>*) <span class="kwrd">this</span>.<span class="hid">reserved.ptr</span>,
<a id="L818" href="#L818" class="br">818 </a>                       <span class="kwrd">this</span>.<span class="hid">reserved.length</span>
<a id="L819" href="#L819" class="br">819 </a>                       ));
<a id="L820" href="#L820" class="br">820 </a>        <span class="kwrd">return</span> <span class="hid">gx</span>;
<a id="L821" href="#L821" class="br">821 </a>    }
<a id="L822" href="#L822" class="br">822 </a>}</pre></div>
        <div id="page-nav">
        <div id="source-navigation"><div class="list-holder"><ul><li><a href="../grain.cudnn.html" class="docs">[Docs] </a><a href="#L1">grain.cudnn</a><ul><li><a href="#L10">grain.cuda</a></li><li><a href="#L11">grain.autograd</a></li><li><a href="#L12">grain.utility</a></li><li><a href="#L13">derelict.cuda</a></li><li><a href="#L14">derelict.cudnn7</a></li><li><a href="#L17">deterministic</a></li><li><a href="#L18">nanProp</a></li><li><a href="../grain.cudnn.isDeterministic.html" class="docs">[Docs] </a><a href="#L21">isDeterministic</a></li><li><a href="../grain.cudnn.isNanProp.html" class="docs">[Docs] </a><a href="#L26">isNanProp</a></li><li><a href="../grain.cudnn.cudnnDataType.html" class="docs">[Docs] </a><a href="#L32">cudnnDataType</a></li><li><a href="../grain.cudnn.TensorDesc.html" class="docs">[Docs] </a><a href="#L40">TensorDesc</a><ul><li><a href="#L41">desc</a></li><li><a href="#L42">ptr</a></li><li><a href="../grain.cudnn.TensorDesc.this(this).html" class="docs">[Docs] </a><a href="#L46">this(this)</a></li><li><a href="../grain.cudnn.TensorDesc.~this.html" class="docs">[Docs] </a><a href="#L50">~this</a></li></ul></li><li><a href="../grain.cudnn.makeCudnnTensor.1.html" class="docs">[Docs] </a><a href="#L56">makeCudnnTensor</a></li><li><a href="../grain.cudnn.makeCudnnTensor.2.html" class="docs">[Docs] </a><a href="#L92">makeCudnnTensor</a></li><li><a href="../grain.cudnn.activationForward.html" class="docs">[Docs] </a><a href="#L111">activationForward</a></li><li><a href="../grain.cudnn.activationBackward.html" class="docs">[Docs] </a><a href="#L136">activationBackward</a></li><li><a href="../grain.cudnn.softmaxForward.html" class="docs">[Docs] </a><a href="#L169">softmaxForward</a></li><li><a href="../grain.cudnn.softmaxBackward.html" class="docs">[Docs] </a><a href="#L184">softmaxBackward</a></li><li><a href="../grain.cudnn.tensorOp.html" class="docs">[Docs] </a><a href="#L215">tensorOp</a></li><li><a href="../grain.cudnn.scale.html" class="docs">[Docs] </a><a href="#L232">scale</a></li><li><a href="../grain.cudnn.reduce.html" class="docs">[Docs] </a><a href="#L254">reduce</a></li><li><a href="../grain.cudnn.fill.html" class="docs">[Docs] </a><a href="#L291">fill</a></li><li><a href="../grain.cudnn.isContiguous.html" class="docs">[Docs] </a><a href="#L296">isContiguous</a></li><li><a href="../grain.cudnn.transform.html" class="docs">[Docs] </a><a href="#L323">transform</a></li><li><a href="#L340">contiguous</a></li><li><a href="../grain.cudnn.convForward.html" class="docs">[Docs] </a><a href="#L429">convForward</a></li><li><a href="../grain.cudnn.convBackward.html" class="docs">[Docs] </a><a href="#L504">convBackward</a></li><li><a href="../grain.cudnn.poolForward.html" class="docs">[Docs] </a><a href="#L595">poolForward</a></li><li><a href="../grain.cudnn.poolBackward.html" class="docs">[Docs] </a><a href="#L665">poolBackward</a></li><li><a href="../grain.cudnn.ThreadLocalDropout.html" class="docs">[Docs] </a><a href="#L733">ThreadLocalDropout</a><ul><li><a href="#L734">count</a></li><li><a href="#L735">_dropoutDesc</a></li><li><a href="#L736">_stateArray</a></li><li><a href="#L738">this(this)</a></li><li><a href="../grain.cudnn.ThreadLocalDropout.init.html" class="docs">[Docs] </a><a href="#L742">init</a></li><li><a href="#L763">descriptor</a></li><li><a href="#L777">state</a></li></ul></li><li><a href="#L783">CudnnDropout</a><ul><li><a href="#L784">reserved</a></li><li><a href="#L786">forward</a></li><li><a href="#L808">backward</a></li></ul></li></ul></li></ul></div></div></div>
    </div>
    <div id="page-footer">
        Distributed under <a href="https://www.boost.org/users/license.html">BSL-1.0</a>.
        Copyright <a href="https://shigekikarita.github.io">Shigeki Karita</a> 2018.
        Page generated by <a href="https://github.com/adamdruppe/adrdox">adrdox</a>
    </div>
</body>
</html>